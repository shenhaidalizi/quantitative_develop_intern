import requests
import time
from datetime import datetime, timedelta, time as dt_time
import json
import os
import pandas as pd
import ast
import glob
from typing import Tuple, Dict, List, Optional, Any
from dataclasses import dataclass
import dotenv
import numpy as np # Added for np.nan and np.sign
import gzip
import pickle

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# ==================== é…ç½®å¸¸é‡ ====================
@dataclass
class TradingConfig:
    """äº¤æ˜“é…ç½®ç±»"""
    # äº¤æ˜“æ—¶é—´æ®µ
    MORNING_START: dt_time = dt_time(9, 31, 0)
    MORNING_END: dt_time = dt_time(11, 30, 0)
    AFTERNOON_START: dt_time = dt_time(13, 0, 0)
    AFTERNOON_END: dt_time = dt_time(15, 0, 0)
    
    # æ—¶é—´å¸¸é‡ï¼ˆç§’ï¼‰
    MINUTE_SECONDS: int = 60
    HOUR_SECONDS: int = 3600
    SLEEP_CHECK_INTERVAL: int = 600  # 10åˆ†é’Ÿ
    
    # æ•°æ®é…ç½®
    MAX_ROLLING_LENGTH: int = 30
    WINDOW_LENGTHS: List[int] = None
    MAX_RESULT_FILES: int = 5
    UPDATE_INTERVAL: int = 60
    
    def __post_init__(self):
        if self.WINDOW_LENGTHS is None:
            self.WINDOW_LENGTHS = [1, 5, 10, 30]

@dataclass
class PathConfig:
    """è·¯å¾„é…ç½®ç±»"""
    # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æ•°æ®æ ¹ç›®å½•
    data_root: str = os.getenv('DATA_ROOT', os.path.dirname(os.path.abspath(__file__)))
    
    @property
    def previous_data_path(self) -> str:
        """åŠ¨æ€è·å–æœ€æ–°çš„æ—¶é—´æ•°æ®æ–‡ä»¶è·¯å¾„"""
        statistic_dir = os.path.join(self.data_root, "statistic_data")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(statistic_dir):
            os.makedirs(statistic_dir)
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ—¶é—´æ•°æ®æ–‡ä»¶ - æ”¹ä¸ºparquetæ ¼å¼
        pattern = os.path.join(statistic_dir, "time_data_*.parquet")
        files = glob.glob(pattern)
        
        if files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(files, key=os.path.getmtime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶: {latest_file}")
            return latest_file
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºé»˜è®¤å€¼
            today_date = datetime.now().strftime('%Y-%m-%d')
            default_file = os.path.join(statistic_dir, f"time_data_{today_date}.parquet")
            print(f"âš ï¸ æœªæ‰¾åˆ°å†å²æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨: {default_file}")
            return default_file
    
    @property
    def save_data_path(self) -> str:
        return os.path.join(self.data_root, "data", "test_result")
    
    @property
    def test_data_path(self) -> str:
        return os.path.join(self.data_root, "data", "test_data")
    
    @property
    def index_data_path(self) -> str:
        return os.path.join(self.data_root, "data", "index_data")
    
    @property
    def index_weight_data_path(self) -> str:
        return os.path.join(self.data_root, "data", "index_weight_data.csv")

    @property
    def test_save_data_path(self) -> str:
        """æµ‹è¯•æ¨¡å¼ä¸“ç”¨ä¿å­˜è·¯å¾„"""
        return os.path.join(self.data_root, "data", "test_result_temp")

# åˆå§‹åŒ–é…ç½®
CONFIG = TradingConfig()
PATHS = PathConfig()

# APIé…ç½®
API_URL = "http://dataapi.trader.com/live/cn/all"

# ==================== Aè‚¡äº¤æ˜“æ—¶é—´æ˜ å°„ç³»ç»Ÿ ====================
def create_trading_time_map() -> Tuple[Dict[str, int], Dict[int, str]]:
    """
    åˆ›å»ºAè‚¡äº¤æ˜“æ—¶é—´æ˜ å°„
    ä½¿ç”¨é…ç½®ä¸­çš„äº¤æ˜“æ—¶é—´æ®µ
    """
    time_to_index: Dict[str, int] = {}
    index_to_time: Dict[int, str] = {}
    index = 0
    
    # ç”Ÿæˆäº¤æ˜“æ—¶é—´æ®µ
    sessions = [
        (CONFIG.MORNING_START, CONFIG.MORNING_END),
        (CONFIG.AFTERNOON_START, CONFIG.AFTERNOON_END)
    ]
    
    for start_time, end_time in sessions:
        current_time = datetime.combine(datetime.today().date(), start_time)
        end_datetime = datetime.combine(datetime.today().date(), end_time)
        
        while current_time < end_datetime:
            time_str = current_time.strftime("%H:%M:%S")
            time_to_index[time_str] = index
            index_to_time[index] = time_str
            index += 1
            current_time += timedelta(minutes=1)
    
    return time_to_index, index_to_time

# åˆ›å»ºå…¨å±€æ˜ å°„
TIME_TO_INDEX, INDEX_TO_TIME = create_trading_time_map()

def time_to_trading_index(time_str: str) -> Optional[int]:
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºäº¤æ˜“åˆ†é’Ÿç´¢å¼•"""
    return TIME_TO_INDEX.get(time_str)

def trading_index_to_time(index: int) -> Optional[str]:
    """å°†äº¤æ˜“åˆ†é’Ÿç´¢å¼•è½¬æ¢ä¸ºæ—¶é—´å­—ç¬¦ä¸²"""
    return INDEX_TO_TIME.get(index)

def get_previous_trading_time(time_str: str, minutes_back: int) -> Optional[str]:
    """è·å–æŒ‡å®šåˆ†é’Ÿæ•°ä¹‹å‰çš„äº¤æ˜“æ—¶é—´"""
    current_index = time_to_trading_index(time_str)
    if current_index is None:
        return None
    
    target_index = max(0, current_index - minutes_back)
    return trading_index_to_time(target_index)

def is_trading_time(time_str: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´"""
    return time_str in TIME_TO_INDEX

def print_trading_map_info() -> None:
    """æ‰“å°äº¤æ˜“æ—¶é—´æ˜ å°„ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
    total_minutes = len(TIME_TO_INDEX)
    if total_minutes > 0:
        print(f"äº¤æ˜“æ—¶é—´æ˜ å°„åˆ›å»ºå®Œæˆï¼Œå…± {total_minutes} ä¸ªäº¤æ˜“åˆ†é’Ÿ")
        first_time = INDEX_TO_TIME.get(0, "N/A")
        last_time = INDEX_TO_TIME.get(total_minutes - 1, "N/A")
        print(f"äº¤æ˜“æ—¶é—´èŒƒå›´: {first_time} - {last_time}")
        print(f"æ€»äº¤æ˜“æ—¶é—´: {total_minutes // 60}å°æ—¶{total_minutes % 60}åˆ†é’Ÿ")

# ==================== æŒ‡æ•°æ–‡ä»¶ç®¡ç† ====================

def read_index_weight_data(path: str):
    """
    return a dataframe with index: con_code(name = None)  col: index_code  weight 
    """
    df = pd.read_csv(path)
    df.drop(columns=['trade_date'], inplace=True)
    df["con_code"] = df["con_code"].str[:-3]
    df.set_index('con_code', inplace=True)
    df.index.name = None
    return df

def manage_index_files(save_path: str, max_files: int = 5):
    """
    ç®¡ç†index_dataæ–‡ä»¶å¤¹ï¼Œåªä¿ç•™æœ€æ–°çš„æŒ‡å®šæ•°é‡çš„æ–‡ä»¶
    
    Args:
        save_path: ä¿å­˜æ–‡ä»¶çš„ç›®å½•è·¯å¾„
        max_files: æœ€å¤§ä¿ç•™æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤5ä¸ª
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(save_path):
            os.makedirs(save_path)
            return
            
        # è·å–ç›®å½•ä¸­æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = []
        for filename in os.listdir(save_path):
            if filename.endswith('.csv'):
                file_path = os.path.join(save_path, filename)
                # è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
                mtime = os.path.getmtime(file_path)
                csv_files.append((mtime, file_path, filename))
        
        # å¦‚æœæ–‡ä»¶æ•°é‡è¶…è¿‡é™åˆ¶
        if len(csv_files) > max_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨åé¢ï¼‰
            csv_files.sort(key=lambda x: x[0])
            
            # åˆ é™¤å¤šä½™çš„æ—§æ–‡ä»¶
            files_to_delete = csv_files[:-max_files]
            
            for _, file_path, filename in files_to_delete:
                os.remove(file_path)
            
    except Exception as e:
        print(f"âš ï¸ æŒ‡æ•°æ–‡ä»¶ç®¡ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

# ==================== åŸæœ‰å‡½æ•° ====================

def manage_result_files(save_path: str, max_files: int = None) -> None:
    """ç®¡ç†ç»“æœæ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°çš„æŒ‡å®šæ•°é‡çš„æ–‡ä»¶ï¼ˆæ”¯æŒ JSON å’Œ CSVï¼‰"""
    if max_files is None:
        max_files = CONFIG.MAX_RESULT_FILES
        
    try:
        # è·å–ç›®å½•ä¸­æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼ˆJSON å’Œ CSVï¼‰
        all_files = []
        for filename in os.listdir(save_path):
            if filename.startswith('test_') and (filename.endswith('.json') or filename.endswith('.csv')):
                file_path = os.path.join(save_path, filename)
                mtime = os.path.getmtime(file_path)
                all_files.append((mtime, file_path, filename))
        
        # æŒ‰æ–‡ä»¶åŸºååˆ†ç»„ï¼ˆåŒä¸€æ—¶é—´ç‚¹çš„ JSON å’Œ CSV æ˜¯ä¸€ç»„ï¼‰
        file_groups = {}
        for mtime, file_path, filename in all_files:
            base_name = filename.rsplit('.', 1)[0]  # å»æ‰æ‰©å±•å
            if base_name not in file_groups:
                file_groups[base_name] = []
            file_groups[base_name].append((mtime, file_path, filename))
        
        # å¦‚æœç»„æ•°è¶…è¿‡é™åˆ¶ï¼Œåˆ é™¤æ—§çš„ç»„
        if len(file_groups) > max_files:
            # æŒ‰æœ€æ–°ä¿®æ”¹æ—¶é—´æ’åºç»„
            sorted_groups = sorted(
                file_groups.items(),
                key=lambda x: max(f[0] for f in x[1])  # å–ç»„å†…æœ€æ–°çš„æ—¶é—´
            )
            
            # åˆ é™¤å¤šä½™çš„æ—§ç»„
            groups_to_delete = sorted_groups[:-max_files]
            
            for base_name, files in groups_to_delete:
                for _, file_path, filename in files:
                    os.remove(file_path)
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ–‡ä»¶: {filename}")
            
            print(f"ğŸ“ æ–‡ä»¶ç®¡ç†å®Œæˆï¼Œä¿ç•™æœ€æ–°çš„ {max_files} ç»„æ–‡ä»¶")
                
    except Exception as e:
        print(f"âš ï¸ æ–‡ä»¶ç®¡ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

def _display_remaining_files(remaining_files: List[Tuple[float, str, str]]) -> None:
    """æ˜¾ç¤ºä¿ç•™çš„æ–‡ä»¶åˆ—è¡¨"""
    print(f"ğŸ“‹ å½“å‰ä¿ç•™æ–‡ä»¶:")
    for i, (mtime, _, filename) in enumerate(remaining_files, 1):
        file_time = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"  {i}. {filename} ({file_time})")

# def load_data_from_parquet(parquet_path: str) -> dict:
#     """ä»Parquetæ–‡ä»¶è¯»å–æ•°æ®å¹¶è½¬æ¢å›åŸæ ¼å¼ï¼ˆè¶…å¿«ç‰ˆæœ¬ï¼‰"""
#     import pyarrow.parquet as pq
    
#     # è¯»å–æ•°æ®
#     table = pq.read_table(parquet_path)
#     df = table.to_pandas()
    
#     # è½¬æ¢æ—¶é—´ä¸ºå­—ç¬¦ä¸²
#     df['time_str'] = df['time'].astype(str)
    
#     # ä½¿ç”¨numpyæ•°ç»„åŠ é€Ÿï¼ˆé¿å…pandasçš„å¼€é”€ï¼‰
#     times = df['time_str'].values
#     stocks = df['stock_code'].values
#     windows = df['window_type'].values
#     means = df['mean'].values
#     stds = df['std'].values
    
#     # æ„å»ºç»“æœå­—å…¸ - å•æ¬¡éå† O(n)
#     final_data = {}
    
#     for i in range(len(df)):
#         time_str = times[i]
#         stock_code = stocks[i]
#         window_type = windows[i]
        
#         if time_str not in final_data:
#             final_data[time_str] = {}
        
#         if stock_code not in final_data[time_str]:
#             final_data[time_str][stock_code] = {}
        
#         final_data[time_str][stock_code][window_type] = {
#             'mean': float(means[i]),
#             'std': float(stds[i])
#         }
    
#     return final_data

def load_parquet_optimized(parquet_path: str):
    """ä¼˜åŒ–ç‰ˆï¼šç›´æ¥è¿”å› DataFrameï¼Œä¸è½¬å­—å…¸"""
    import pyarrow.parquet as pq
    table = pq.read_table(parquet_path)
    df = table.to_pandas()
    df['time'] = df['time'].astype(str)
    return df  # è¿”å›åŸå§‹ DataFrame

# def read_previous_data(previous_path: str) -> Dict[str, Any]:
#     """è¯»å–å†å²æ•°æ®æ–‡ä»¶ - æ”¯æŒparquetå’Œjsonæ ¼å¼ï¼ˆè¿”å›å­—å…¸æ ¼å¼ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰"""
#     if previous_path.endswith('.parquet'):
#         return load_data_from_parquet(previous_path)
#     elif previous_path.endswith('.json'):
#         with open(previous_path, 'r', encoding='utf-8') as f:
#             return json.load(f)
#     else:
#         raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {previous_path}")

def read_previous_data_optimized(previous_path: str) -> pd.DataFrame:
    """è¯»å–å†å²æ•°æ®æ–‡ä»¶ - ä¼˜åŒ–ç‰ˆï¼Œç›´æ¥è¿”å›DataFrame"""
    # å­˜åœ¨æ€§æ ¡éªŒï¼Œæå‡å¯è§‚æµ‹æ€§
    if not os.path.exists(previous_path):
        raise FileNotFoundError(f"ç»Ÿè®¡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {previous_path}")
    if previous_path.endswith('.parquet'):
        return load_parquet_optimized(previous_path)
    elif previous_path.endswith('.json'):
        # JSONæ ¼å¼ä¸æ”¯æŒä¼˜åŒ–ï¼šæç¤ºä½¿ç”¨parquetæ ¼å¼ï¼Œé¿å…æœªå®šä¹‰å‡½æ•°å¼•ç”¨
        raise ValueError("JSONæ ¼å¼ä¸æ”¯æŒä¼˜åŒ–ï¼Œè¯·ä½¿ç”¨Parquetç»Ÿè®¡æ•°æ®æ–‡ä»¶ (time_data_*.parquet)")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {previous_path}")

def get_certain_time_data_optimized(parquet_df: pd.DataFrame, time: str) -> pd.DataFrame:
    """ç›´æ¥æŸ¥è¯¢ Parquet DataFrame"""
    # ç­›é€‰ç‰¹å®šæ—¶é—´çš„æ•°æ®
    time_data = parquet_df[parquet_df['time'] == time].copy()
    
    # Pivot: ä»é•¿æ ¼å¼è½¬å®½æ ¼å¼
    result = time_data.pivot(
        index='stock_code', 
        columns='window_type', 
        values=['mean', 'std']
    )
    
    # æ‰å¹³åŒ–åˆ—åï¼š('mean', 'rolling1') -> 'rolling1_mean'
    result.columns = [f'{col[1]}_{col[0]}' for col in result.columns]
    
    # ========== ä¿®å¤ï¼šæ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆå»æ‰åç¼€ï¼‰==========
    # å¦‚æœç´¢å¼•åŒ…å« .SZ/.SH/.BJ åç¼€ï¼Œå»æ‰å®ƒä»¬ä»¥åŒ¹é… rolling_result çš„æ ¼å¼
    if any('.' in str(idx) for idx in result.index):
        result.index = result.index.str.replace(r'\.(SZ|SH|BJ)$', '', regex=True)
        print(f"  ğŸ”§ å·²æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆå»é™¤åç¼€ï¼‰ï¼Œå…± {len(result)} åªè‚¡ç¥¨")
    # ========================================================
    
    return result

def read_snapshot_data(snapshot_path: str, analysis_variable: str) -> pd.DataFrame:
    """è¯»å–å¿«ç…§æ•°æ®"""
    snapshot_data = pd.json_normalize(pd.read_json(snapshot_path)["data"])
    snapshot_data = snapshot_data[['Symbol', analysis_variable]].set_index('Symbol')
    snapshot_data.index.name = None
    return snapshot_data

def get_data(snapshot_data: pd.DataFrame, previous_data: pd.DataFrame) -> pd.DataFrame:
    """åˆå¹¶å¿«ç…§æ•°æ®å’Œå†å²æ•°æ®"""
    # å±•å¼€å­—å…¸ç±»å‹çš„åˆ—
    for col_name in previous_data.columns:
        if col_name in previous_data.columns and isinstance(previous_data[col_name].iloc[0], dict):
            expanded_df = previous_data[col_name].apply(pd.Series)
            expanded_df.columns = [f'{col_name}_{sub_col}' for sub_col in expanded_df.columns]
            previous_data = pd.concat([previous_data, expanded_df], axis=1).drop(columns=[col_name])
    
    print("--------------------------------")
    return snapshot_data.join(previous_data, how='inner')

def get_z_score(final_data: pd.DataFrame, window_length_list: list, save_path: str) -> pd.DataFrame: 
    """è®¡ç®—Zåˆ†æ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šå¤„ç†æ ‡å‡†å·®ä¸º0çš„æƒ…å†µï¼‰"""
    print(f"window_length_list: {window_length_list}")

    # ğŸ‘‡ ç”¨æ¥å­˜å‚¨è°ƒè¯•ä¿¡æ¯çš„åˆ—è¡¨
    debug_records = []
    # æ—¶é—´æˆ³ï¼ˆå»æ‰å†’å·ï¼Œé¿å…Windowsè·¯å¾„é—®é¢˜ï¼‰
    time_str = time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime()).replace(":", "")

    for length in window_length_list:
        mean_col = f'rolling{length}_mean'
        std_col = f'rolling{length}_std'
        rolling_col = f"rolling{length}"
        z_col = f'rolling{length}_z_score'
        
        if all(col in final_data.columns for col in [rolling_col, mean_col, std_col]):
            std_values = final_data[std_col].copy()
            std_values = std_values.replace(0, np.nan)  # å°†0æ›¿æ¢ä¸ºNaN
            
            # è®¡ç®—z-scoreï¼Œæ ‡å‡†å·®ä¸º0æ—¶ä¿æŒåŸå§‹åå·®å€¼
            z_scores = (final_data[rolling_col] - final_data[mean_col]) / std_values
            
            # å¯¹äºstdä¸º0çš„æƒ…å†µï¼Œå¦‚æœåå·®ä¹Ÿä¸º0åˆ™è®¾ä¸º0ï¼Œå¦åˆ™è®¾ä¸ºÂ±5ï¼ˆæç«¯å€¼ï¼‰
            zero_std_mask = final_data[std_col] == 0
            z_scores[zero_std_mask] = 0
            final_data[z_col] = z_scores.round(2)

             # ğŸ‘‡ æ”¶é›†è°ƒè¯•ä¿¡æ¯
            for idx, row in final_data.iterrows():
                debug_records.append({
                    "code": row['code'] if 'code' in final_data.columns else idx,
                    "window": length,
                    "rolling": row[rolling_col],
                    "mean": row[mean_col],
                    "std": row[std_col],
                    "z_score": row[z_col]
                })

    # ä¿å­˜ debug æ–‡ä»¶
    if debug_records:
        debug_df = pd.DataFrame(debug_records)
        debug_file_path = os.path.join(save_path, f'z_score_{time_str}.csv')
        debug_df.to_csv(debug_file_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ z_score_debug å·²ä¿å­˜ä¸º CSV: {debug_file_path}")

    useful_columns = [f'rolling{length}' for length in window_length_list] + \
                    [f'rolling{length}_mean' for length in window_length_list] + \
                    [f'rolling{length}_z_score' for length in window_length_list]
    
    return final_data[useful_columns]

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from typing import List

# def get_z_score(final_data: pd.DataFrame, window_length_list: List[int], visualize: bool = True) -> pd.DataFrame:
#     """
#     è®¡ç®—Zåˆ†æ•°ï¼ˆä¸å†éšæœºï¼Œè¾“å‡ºæ‰€æœ‰è¡Œçš„è®¡ç®—è¿‡ç¨‹ï¼‰
#     - visualize: æ˜¯å¦å¯¹æ¯ä¸ªwindowç”»å‡ºå¯è§†åŒ–å¯¹æ¯”
#     """
#     print(f"ğŸ“Š å¼€å§‹è®¡ç®— Z-scoreï¼Œçª—å£åˆ—è¡¨: {window_length_list}")

#     for length in window_length_list:
#         rolling_col = f'rolling{length}'
#         mean_col = f'rolling{length}_mean'
#         std_col = f'rolling{length}_std'
#         z_col = f'rolling{length}_z_score'

#         # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
#         if not all(col in final_data.columns for col in [rolling_col, mean_col, std_col]):
#             print(f"âš ï¸ ç¼ºå°‘åˆ—ï¼Œè·³è¿‡ {length}: {[c for c in [rolling_col, mean_col, std_col] if c not in final_data.columns]}")
#             continue

#         # å¤åˆ¶ stdï¼Œé¿å…ä¿®æ”¹åŸè¡¨
#         std_values = final_data[std_col].copy().replace(0, np.nan)

#         # è®¡ç®— z-score
#         z_scores = (final_data[rolling_col] - final_data[mean_col]) / std_values

#         # std == 0 çš„æƒ…å†µå¤„ç†
#         zero_std_mask = final_data[std_col] == 0
#         z_scores[zero_std_mask] = 0

#         # ä¿ç•™ä¸¤ä½å°æ•°
#         final_data[z_col] = z_scores.round(2)

#         # ğŸ“ è¾“å‡ºæ‰€æœ‰è¡Œçš„è¯¦ç»†è®¡ç®—è¿‡ç¨‹
#         print(f"\nğŸ“Œ [rolling{length}] å…¨éƒ¨è¡Œè®¡ç®—è¿‡ç¨‹ï¼š")
#         for idx, row in final_data.iterrows():
#             print(
#                 f"  è¡Œ {idx}: rolling={row[rolling_col]:.4f}, "
#                 f"mean={row[mean_col]:.4f}, std={row[std_col]:.4f}, "
#                 f"z={row[z_col]:.4f}"
#             )

#         # ğŸ“ˆ å¯è§†åŒ–
#         if visualize:
#             plt.figure(figsize=(10, 4))
#             x = np.arange(len(final_data))
#             plt.plot(x, final_data[rolling_col], label='rolling', marker='o')
#             plt.plot(x, final_data[mean_col], label='mean', marker='x')
#             plt.plot(x, final_data[std_col], label='std', marker='s')
#             plt.plot(x, final_data[z_col], label='z_score', marker='^')
#             plt.title(f'Z-Score è®¡ç®—å¯è§†åŒ– (window={length})')
#             plt.xlabel('è¡Œç´¢å¼•')
#             plt.legend()
#             plt.grid(True)
#             plt.show()

#     # è¿”å›åŒ…å« rolling / mean / z_score çš„åˆ—
#     useful_columns = []
#     for length in window_length_list:
#         useful_columns += [f'rolling{length}', f'rolling{length}_mean', f'rolling{length}_z_score']

#     return final_data[useful_columns]



def fetch_minute_data(url: str) -> Optional[Dict[str, Any]]:
    """è·å–åˆ†é’Ÿçº§æ•°æ®"""
    try:
        response = requests.get(url, timeout=10,proxies={"http":None,"https":None})
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None

def _fetch_cumulative_volume_map(api_url: str) -> Dict[str, float]:
    """ä»å®æ—¶HTTPæ¥å£è·å–å½“å‰ç´¯ç§¯æˆäº¤é‡ï¼ˆæŒ‰Symbolï¼‰ã€‚è·å–å¤±è´¥è¿”å›ç©ºå­—å…¸ã€‚"""
    raw = fetch_minute_data(api_url)
    if not raw or "data" not in raw:
        return {}
    # å¤ç”¨å·²æœ‰è½¬æ¢é€»è¾‘ï¼šSymbol -> [Name, Latest, ChangePercent, TradingVolume(ç´¯è®¡)]
    transformed = _transform_market_data(raw)
    return {
        symbol: float(values[3])  # TradingVolume ä¸ºç´¯è®¡é‡
        for symbol, values in transformed.items()
        if values is not None and len(values) >= 4
    }

# æ•°æ®ç»“æ„è¯´æ˜: 0:è‚¡ç¥¨åç§° 1:æœ€æ–°ä»· 2:æ¶¨è·Œå¹… 3:äº¤æ˜“é‡ 4:äº”åˆ†é’Ÿä»·æ ¼å˜åŒ– 5:30åˆ†é’Ÿä»·æ ¼å˜åŒ–

def calculate_rolling(rolling: Dict[str, Dict[str, List]], key: str, window_lengths: List[int], certain_time_data: pd.DataFrame = None) -> pd.DataFrame:
    """è®¡ç®—rollingæ•°æ®ï¼Œä½¿ç”¨äº¤æ˜“æ—¶é—´æ˜ å°„è€ƒè™‘Aè‚¡äº¤æ˜“æ—¶é—´è¿ç»­æ€§"""
    print(f"ğŸ”„ å¼€å§‹è®¡ç®—rollingæ•°æ®ï¼Œå½“å‰æ—¶é—´: {key}")
    
    # æå–äº¤æ˜“é‡æ•°æ® - ä¿®å¤ï¼šä½¿ç”¨åˆ†é’Ÿäº¤æ˜“é‡ï¼Œå¹¶ä¿æŒä¸é¢„å¤„ç†æ•°æ®ä¸€è‡´çš„å•ä½
    volume_data = {}
    for time, stocks in rolling.items():
        volume_data[time] = {}
        for stock_code, stock_data in stocks.items():
            # stock_data[3] æ˜¯ [åˆ†é’Ÿäº¤æ˜“é‡, ç´¯è®¡äº¤æ˜“é‡] æˆ–è€…æ˜¯å•ä¸ªå€¼
            if isinstance(stock_data[3], list):
                minute_volume = stock_data[3][0]  # åˆ†é’Ÿäº¤æ˜“é‡
            else:
                minute_volume = stock_data[3]
            
            
            volume_data[time][stock_code] = minute_volume
    
    df = pd.DataFrame.from_dict(volume_data, orient='index')
    ordered_times = _get_ordered_trading_times(df.index, key)
    df = df.reindex(ordered_times)
    
    current_idx = ordered_times.index(key)
    current_trading_index = time_to_trading_index(key)
    print(f"ğŸ“Š å¯ç”¨æ—¶é—´ç‚¹: {len(ordered_times)}, å½“å‰ä½ç½®: {current_idx}, äº¤æ˜“ç´¢å¼•: {current_trading_index}")
    
    return _calculate_rolling_windows(df, key, ordered_times, current_idx, current_trading_index, window_lengths, certain_time_data)

def _get_ordered_trading_times(time_indices: List[str], current_key: str) -> List[str]:
    """è·å–æŒ‰äº¤æ˜“æ—¶é—´ç´¢å¼•æ’åºçš„æ—¶é—´åˆ—è¡¨"""
    time_index_pairs = [
        (time, time_to_trading_index(time)) 
        for time in time_indices 
        if time_to_trading_index(time) is not None
    ]
    time_index_pairs.sort(key=lambda x: x[1])
    return [pair[0] for pair in time_index_pairs]

def get_realtime_trading_volume_sum() -> float:
    
    url = "http://dataapi.trader.com/live/cn/all"
    try:
        resp = requests.get(url, timeout=5)
        resp.raise_for_status()
        data_json = resp.json()

        # ç¡®ä¿ç»“æ„æ­£ç¡®
        if "data" in data_json and isinstance(data_json["data"], list):
            trading_volumes = [item.get("TradingVolume", 0) for item in data_json["data"]]
            total_volume = sum(trading_volumes)
            return total_volume
        else:
            print("è¿”å› JSON ç»“æ„å¼‚å¸¸:", data_json)
            return 0.0

    except Exception as e:
        print(f"è·å–å®æ—¶æ•°æ®å¤±è´¥: {e}")
        return 0.0

def _calculate_rolling_windows(df: pd.DataFrame, key: str, ordered_times: List[str], 
                             current_idx: int, current_trading_index: int, 
                             window_lengths: List[int], certain_time_data: pd.DataFrame = None) -> pd.DataFrame:
    """è®¡ç®—å„ä¸ªçª—å£é•¿åº¦çš„rollingæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    result = {}
    
    # è®¡ç®—ä»å¼€ç›˜åˆ°å½“å‰çš„ç´¯ç§¯äº¤æ˜“é‡ï¼ˆrolling_fullï¼‰
    current_data = df.loc[ordered_times[:current_idx + 1]]
    print(current_data)
    if not current_data.empty:
        # ä¼˜å…ˆç”¨ HTTP æ¥å£ç´¯è®¡æˆäº¤é‡ï¼ˆæŒ‰ Symbolï¼‰ï¼Œå¤±è´¥å›é€€ä¸ºåˆ†é’Ÿé‡ç´¯åŠ 
        api_cum_map = _fetch_cumulative_volume_map(API_URL)
        if api_cum_map:
            cumulative_series = pd.Series(api_cum_map).reindex(df.columns)
            result['rolling_full_sum'] = cumulative_series
        else:
            cumulative_sum = current_data.sum()
            result['rolling_full_sum'] = cumulative_sum
        
            # å¦‚æœæä¾›äº†statistic data
            if certain_time_data is not None and not certain_time_data.empty:
                # ç›´æ¥æ£€æŸ¥æ˜¯å¦æœ‰ rolling_full åˆ—
                rolling_full_cols = [col for col in certain_time_data.columns if 'rolling_full' in col]
                if rolling_full_cols:
                    rolling_full_mean_col = [col for col in rolling_full_cols if 'mean' in col]
                    rolling_full_std_col = [col for col in rolling_full_cols if 'std' in col]
                    
                    if rolling_full_mean_col and rolling_full_std_col:
                        # è·å–å†å²ç»Ÿè®¡æ•°æ®
                        preprocess_sum_all = certain_time_data[rolling_full_mean_col[0]]
                        preprocess_std_all = certain_time_data[rolling_full_std_col[0]]
                        
                        # ç»Ÿä¸€è‚¡ç¥¨ä»£ç æ ¼å¼ï¼šå°è¯•åŒ¹é…å¸¦åç¼€å’Œä¸å¸¦åç¼€çš„æ ¼å¼
                        # cumulative_sum çš„ index å¯èƒ½æ˜¯ '000001'
                        # preprocess çš„ index å¯èƒ½æ˜¯ '000001.SZ' æˆ– '000001.SH'
                        
                        # åˆ›å»ºæ˜ å°„ï¼šå»æ‰åç¼€
                        preprocess_sum_normalized = preprocess_sum_all.copy()
                        preprocess_std_normalized = preprocess_std_all.copy()
                        
                        # å¦‚æœé¢„å¤„ç†æ•°æ®æœ‰åç¼€ï¼Œåˆ›å»ºä¸å¸¦åç¼€çš„ç´¢å¼•
                        if any('.' in str(idx) for idx in preprocess_sum_all.index):
                            # å»æ‰ .SZ/.SH åç¼€
                            new_index = preprocess_sum_all.index.str.replace(r'\.(SZ|SH|BJ)$', '', regex=True)
                            preprocess_sum_normalized.index = new_index
                            preprocess_std_normalized.index = new_index
                        
                        # æ‰¾åˆ°å…±åŒçš„è‚¡ç¥¨ä»£ç 
                        common_stocks = cumulative_sum.index.intersection(preprocess_sum_normalized.index)
                        
                        if len(common_stocks) > 0:
                            # å¯¹é½æ‰€æœ‰ Series åˆ°ç›¸åŒçš„ç´¢å¼•
                            cumulative_sum_aligned = cumulative_sum.loc[common_stocks]
                            preprocess_sum = preprocess_sum_normalized.loc[common_stocks]
                            preprocess_std = preprocess_std_normalized.loc[common_stocks]
                            
                            # è®¡ç®—åå·®
                            diff = cumulative_sum_aligned - preprocess_sum
                            
                            # åˆå§‹åŒ– z_scores
                            z_scores = pd.Series(0.0, index=common_stocks, dtype=float)
                            
                            # å¤„ç†æ ‡å‡†å·® > 0 çš„æƒ…å†µ
                            mask_valid_std = preprocess_std > 1e-6
                            if mask_valid_std.sum() > 0:
                                valid_stocks = mask_valid_std[mask_valid_std].index
                                z_scores.loc[valid_stocks] = (
                                    diff.loc[valid_stocks] / preprocess_std.loc[valid_stocks]
                                ).round(2)
                            
                            # å¤„ç†æ ‡å‡†å·® â‰ˆ 0 çš„æƒ…å†µ
                            mask_zero_std = preprocess_std <= 1e-6
                            if mask_zero_std.sum() > 0:
                                zero_stocks = mask_zero_std[mask_zero_std].index
                                z_scores.loc[zero_stocks] = np.where(
                                    abs(diff.loc[zero_stocks]) < 1e-6,
                                    0,  # å‡ ä¹æ²¡æœ‰åå·®
                                    np.sign(diff.loc[zero_stocks]) * 3  # æœ‰æ˜æ˜¾åå·®
                                )
                            
                            result['rolling_full'] = z_scores
                            print(f"  ğŸ“ˆ rolling_full: è®¡ç®—äº† {len(common_stocks)} åªè‚¡ç¥¨çš„z-score")
                        else:
                            result['rolling_full'] = pd.Series(np.nan, index=cumulative_sum.index)
                            print(f"  âš ï¸ rolling_full: æ²¡æœ‰åŒ¹é…çš„è‚¡ç¥¨ä»£ç ï¼ˆæ ¼å¼å¯èƒ½ä¸ä¸€è‡´ï¼‰")
                    else:
                        result['rolling_full'] = pd.Series(np.nan, index=cumulative_sum.index)
                        print(f"  ğŸ“ˆ rolling_full: æœªæ‰¾åˆ°å®Œæ•´é¢„å¤„ç†æ•°æ®åˆ—")
                else:
                    result['rolling_full'] = pd.Series(np.nan, index=cumulative_sum.index)
                    print(f"  ğŸ“ˆ rolling_full: æœªæ‰¾åˆ°rolling_fullåˆ—")
            else:
                result['rolling_full'] = pd.Series(np.nan, index=cumulative_sum.index)
                print(f"  âš ï¸ rolling_full: æœªæä¾›é¢„å¤„ç†æ•°æ®ï¼Œè®¾ä¸ºNaN")
    

    # è®¡ç®—å…¶ä»–æ»šåŠ¨çª—å£
    for window in window_lengths:
        col_name = f'rolling{window}'
        
        available_minutes = current_idx + 1
        
        if available_minutes <= window:
            window_data = current_data
            print(f"  ğŸ“ˆ rolling{window}: æ—¶é—´ä¸è¶³{window}åˆ†é’Ÿï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ® ({available_minutes} åˆ†é’Ÿ)")
        else:
            start_time = get_previous_trading_time(key, window - 1)
            if start_time is None:
                window_data = current_data
                print(f"  ğŸ“ˆ rolling{window}: æ— æ³•è·å–å†å²æ—¶é—´ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®")
            else:
                window_times = [
                    time_str for time_str in ordered_times[:current_idx + 1]
                    if time_to_trading_index(time_str) is not None
                    and time_to_trading_index(time_str) >= time_to_trading_index(start_time)
                ]
                window_data = df.loc[window_times] if window_times else current_data
                print(f"  ğŸ“ˆ rolling{window}: ä½¿ç”¨æ—¶é—´çª—å£ {start_time} - {key} ({len(window_times)} ä¸ªæ—¶é—´ç‚¹)")
        
        rolling_sum = window_data.sum()
        result[col_name] = rolling_sum.round(2)
    
    return pd.DataFrame(result)



def get_time_status_and_sleep(test_mode: bool = False) -> Tuple[str, float, str, str]:
    """
    è·å–å½“å‰æ—¶é—´çŠ¶æ€å’Œç¡çœ ä¿¡æ¯
    è¿”å›: (status, wait_seconds, next_session, message)
    """
    now = datetime.now()
    current_time = now.time()
    time_str = now.strftime('%H:%M:%S')
    
    # æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œæ€»æ˜¯è¿”å›äº¤æ˜“çŠ¶æ€
    if test_mode:
        return 'trading', 0.0, "æµ‹è¯•æ¨¡å¼", f"ğŸ§ª æµ‹è¯•æ¨¡å¼: å½“å‰æ—¶é—´ {time_str} (å¿½ç•¥äº¤æ˜“æ—¶é—´é™åˆ¶)"
    
    # åˆ¤æ–­æ—¶é—´çŠ¶æ€å¹¶è®¡ç®—ç­‰å¾…æ—¶é—´
    if current_time < CONFIG.MORNING_START:
        target_time = datetime.combine(now.date(), CONFIG.MORNING_START)
        wait_seconds = (target_time - now).total_seconds()
        return 'wait', wait_seconds, "æ—©ç›˜å¼€ç›˜", f"ğŸŒ… å½“å‰æ—¶é—´ {time_str} å¼€ç›˜å‰ï¼Œç­‰å¾…å¼€ç›˜..."
        
    elif CONFIG.MORNING_END < current_time < CONFIG.AFTERNOON_START:
        target_time = datetime.combine(now.date(), CONFIG.AFTERNOON_START)
        wait_seconds = (target_time - now).total_seconds()
        return 'wait', wait_seconds, "åˆç›˜å¼€ç›˜", f"ğŸ½ï¸ å½“å‰æ—¶é—´ {time_str} ä¸­åˆä¼‘å¸‚ï¼Œç­‰å¾…ä¸‹åˆå¼€ç›˜..."
        
    elif current_time >= CONFIG.AFTERNOON_END:
        tomorrow = now.date() + timedelta(days=1)
        target_time = datetime.combine(tomorrow, CONFIG.MORNING_START)
        wait_seconds = (target_time - now).total_seconds()
        return 'exit', wait_seconds, "æ˜æ—¥æ—©ç›˜å¼€ç›˜", f"ğŸŒ… å½“å‰æ—¶é—´ {time_str} æ”¶ç›˜åï¼Œç¨‹åºå°†é€€å‡º..."
        
    else:
        return 'trading', 0.0, "äº¤æ˜“æ—¶é—´å†…", f"âœ… å½“å‰æ—¶é—´ {time_str} äº¤æ˜“æ—¶é—´"

def format_time_duration(seconds: float) -> Tuple[int, int, int]:
    """æ ¼å¼åŒ–æ—¶é—´æŒç»­æ—¶é—´ä¸ºæ—¶åˆ†ç§’"""
    return int(seconds // CONFIG.HOUR_SECONDS), int((seconds % CONFIG.HOUR_SECONDS) // CONFIG.MINUTE_SECONDS), int(seconds % CONFIG.MINUTE_SECONDS)

def handle_sleep(wait_seconds: float, next_session: str) -> None:
    """å¤„ç†ç¡çœ é€»è¾‘çš„è¾…åŠ©å‡½æ•°"""
    hours, minutes, seconds = format_time_duration(wait_seconds)
    
    print(f"ğŸ’¤ ç¨‹åºè¿›å…¥ç¡çœ æ¨¡å¼ï¼Œç­‰å¾… {next_session}")
    print(f"ğŸ’¤ é¢„è®¡ç­‰å¾…æ—¶é—´: {hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’")
    print(f"ğŸ’¤ å°†åœ¨ {(datetime.now() + timedelta(seconds=wait_seconds)).strftime('%Y-%m-%d %H:%M:%S')} æ¢å¤è¿è¡Œ")
    
    # é•¿æ—¶é—´ç¡çœ æ—¶å®šæœŸæ£€æŸ¥
    if wait_seconds > CONFIG.HOUR_SECONDS:
        _handle_long_sleep(wait_seconds)
    else:
        time.sleep(wait_seconds)
    
    print(f"â° ç¡çœ ç»“æŸï¼Œç¨‹åºæ¢å¤è¿è¡Œ")

def _handle_long_sleep(wait_seconds: float) -> None:
    """å¤„ç†é•¿æ—¶é—´ç¡çœ ï¼Œå®šæœŸæ˜¾ç¤ºè¿›åº¦"""
    total_sleep = 0.0
    while total_sleep < wait_seconds:
        remaining_sleep = min(CONFIG.SLEEP_CHECK_INTERVAL, wait_seconds - total_sleep)
        time.sleep(remaining_sleep)
        total_sleep += remaining_sleep
        
        if total_sleep % CONFIG.SLEEP_CHECK_INTERVAL == 0 and total_sleep < wait_seconds:
            remaining_time = wait_seconds - total_sleep
            remaining_hours, remaining_minutes = format_time_duration(remaining_time)[:2]
            print(f"ğŸ’¤ ç»§ç»­ç­‰å¾…ï¼Œå‰©ä½™æ—¶é—´: {remaining_hours}å°æ—¶{remaining_minutes}åˆ†é’Ÿ")
    
def create_data_structure(current_data: Dict[str, List], rolling: Dict[str, Dict], time_str: str, 
                        start_time: str) -> Dict[str, List]:
    """åˆ›å»ºå½“å‰æ—¶é—´çš„æ•°æ®ç»“æ„"""
    if not rolling:
        # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
        return {
            symbol: [
                current_data[symbol][0],  # è‚¡ç¥¨åç§°
                current_data[symbol][1],  # æœ€æ–°ä»·
                current_data[symbol][2],  # æ¶¨è·Œå¹…
                [current_data[symbol][3], current_data[symbol][3]],  # [åˆ†é’Ÿäº¤æ˜“é‡, ç´¯è®¡äº¤æ˜“é‡]
                0,  # äº”åˆ†é’Ÿä»·æ ¼å˜åŒ–
                0   # 30åˆ†é’Ÿä»·æ ¼å˜åŒ–
            ]
            for symbol in current_data
        }
    else:
        # è·å–å†å²æ—¶é—´ç‚¹
        last_min_time = get_previous_trading_time(time_str, 1) or start_time
        five_min_time = get_previous_trading_time(time_str, 5) or start_time
        thirty_min_time = get_previous_trading_time(time_str, 30) or start_time
        
        print(f"ğŸ“… å†å²æ—¶é—´ç‚¹: 1åˆ†é’Ÿå‰={last_min_time}, 5åˆ†é’Ÿå‰={five_min_time}, 30åˆ†é’Ÿå‰={thirty_min_time}")
        
        # è·å–å†å²æ•°æ®
        last_min_data = rolling.get(last_min_time, rolling[start_time])
        five_min_data = rolling.get(five_min_time, rolling[start_time])
        thirty_min_data = rolling.get(thirty_min_time, rolling[start_time])
        
        return {
            symbol: [
                current_data[symbol][0],  # è‚¡ç¥¨åç§°
                current_data[symbol][1],  # æœ€æ–°ä»·
                current_data[symbol][2],  # æ¶¨è·Œå¹…
                [  # [åˆ†é’Ÿäº¤æ˜“é‡, ç´¯è®¡äº¤æ˜“é‡]
                    current_data[symbol][3] - last_min_data[symbol][3][1] if symbol in last_min_data else current_data[symbol][3],
                    current_data[symbol][3]
                ],
                # äº”åˆ†é’Ÿä»·æ ¼å˜åŒ–
                ((current_data[symbol][1] - five_min_data[symbol][1]) / five_min_data[symbol][1] * 100 
                 if symbol in five_min_data and five_min_data[symbol][1] != 0 else 0),
                # 30åˆ†é’Ÿä»·æ ¼å˜åŒ–
                ((current_data[symbol][1] - thirty_min_data[symbol][1]) / thirty_min_data[symbol][1] * 100 
                 if symbol in thirty_min_data and thirty_min_data[symbol][1] != 0 else 0)
            ]
            for symbol in current_data
        }

def process_data_and_save(rolling: Dict, time_str: str, statistics_data: Dict, save_path: str, index_weight_data: pd.DataFrame) -> None:
    """å¤„ç†æ•°æ®å¹¶ä¿å­˜æ–‡ä»¶"""
    current_trading_index = time_to_trading_index(time_str)
    
    # è·å–å†å²ç»Ÿè®¡æ•°æ®ï¼ˆå…ˆå®šä¹‰ï¼‰
    certain_time_data = get_certain_time_data_optimized(statistics_data, time_str)
    
    # è®¡ç®—rollingç»“æœï¼ˆåä½¿ç”¨ï¼‰
    rolling_result = calculate_rolling(rolling, time_str, CONFIG.WINDOW_LENGTHS, certain_time_data)

    # ä¿å­˜ rolling_result ä¸º CSV æ–‡ä»¶ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
    rolling_csv_path = os.path.join(save_path, f'rolling_result_{time_str.replace(":", "")}.csv')
    os.makedirs(save_path, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    rolling_result.to_csv(rolling_csv_path, index=True)  # ä¿å­˜ç´¢å¼•
    print(f"ğŸ’¾ rolling_result å·²ä¿å­˜ä¸º CSV: {rolling_csv_path}")
    rolling_dict_csv_path = os.path.join(save_path, f'rolling_{time_str.replace(":", "")}.csv')
    os.makedirs(save_path, exist_ok=True)

    # å°†åµŒå¥—å­—å…¸è½¬ä¸º DataFrame
    rolling_df = pd.DataFrame.from_dict(rolling[time_str], orient='index',
                                    columns=["Name", "Price", "Chg", "Vol", "Chg5", "Chg30"])
    rolling_df.index.name = "Symbol"  # è‚¡ç¥¨ä»£ç ä½œä¸ºç´¢å¼•
    # rolling_df.to_csv(rolling_dict_csv_path)
    # print(f"ğŸ’¾ rolling å·²ä¿å­˜ä¸º CSV: {rolling_dict_csv_path}")

    
    # åˆ›å»ºä¿å­˜æ•°æ®çš„DataFrame
    index_list = ["Name", "Price", "Chg", "Vol", "Chg5", "Chg30"]
    save_data = {
        symbol: [
            rolling[time_str][symbol][0],  # Name
            rolling[time_str][symbol][1],  # Price
            rolling[time_str][symbol][2],  # Chg
            rolling_result.loc[symbol, 'rolling_full_sum'],  # Vol: ä½¿ç”¨åˆ†é’Ÿæˆäº¤é‡ï¼Œä¿æŒä¸é¢„å¤„ç†ä¸€è‡´çš„å•ä½
            rolling[time_str][symbol][4],  # Chg5
            rolling[time_str][symbol][5]   # Chg30
        ]
        for symbol in rolling[time_str]
    }
    info = pd.DataFrame(save_data, index=index_list).T

    # # âœ… ä¿å­˜ info ä¸º CSV
    # info_csv_path = os.path.join(save_path, f'info_{time_str.replace(":", "")}.csv')
    # info.to_csv(info_csv_path, index=True)
    # print(f"ğŸ’¾ info å·²ä¿å­˜ä¸º CSV: {info_csv_path}")
    
    # å†å²ç»Ÿè®¡æ•°æ®å·²åœ¨å‰é¢è·å–
    # åˆå¹¶æ•°æ®å¹¶è®¡ç®—Zåˆ†æ•°
    z_score_data = rolling_result.join(certain_time_data, how='inner')
    z_score_data = get_z_score(z_score_data, CONFIG.WINDOW_LENGTHS, save_path)
    
    # å¦‚æœå­˜åœ¨rolling_fullï¼ˆä»å¼€ç›˜åˆ°ç°åœ¨çš„z-scoreï¼‰ï¼Œæ·»åŠ åˆ°z_score_dataä¸­
    if 'rolling_full' in rolling_result.columns:
        z_score_data['rolling_full_z_score'] = rolling_result['rolling_full']
    
     # âœ… ä¿å­˜ z_score_data ä¸º CSV
    z_score_csv_path = os.path.join(save_path, f'z_score_{time_str.replace(":", "")}.csv')
    z_score_data.to_csv(z_score_csv_path, index=True)
    print(f"ğŸ’¾ z_score_data å·²ä¿å­˜ä¸º CSV: {z_score_csv_path}")


    # åˆ›å»ºæœ€ç»ˆæ•°æ®
    useful_columns = ["Name", "Price", "Chg", "Vol"] + \
                    [f'rolling{length}_z_score' for length in CONFIG.WINDOW_LENGTHS] + \
                    ['rolling_full_z_score'] + \
                    ["Chg5", "Chg30"]
    final_data = info.join(z_score_data, how="inner")[useful_columns]
    final_data.sort_values(by="rolling5_z_score", ascending=False, inplace=True)

    # âœ… ä¿å­˜ final_data ä¸º CSV
    final_csv_path = os.path.join(save_path, f'final_data_{time_str.replace(":", "")}.csv')
    final_data.to_csv(final_csv_path, index=True)
    print(f"ğŸ’¾ final_data å·²ä¿å­˜ä¸º CSV: {final_csv_path}")


    # è®¡ç®—æŒ‡æ•°
    join_index_data = index_weight_data.join(final_data, how='left')
    process_col = final_data.columns[2:]  # çœç•¥æ‰Nameåˆ—
    useful_col = []
    
    for col in process_col:
        join_index_data[f"index_{col}"] = (join_index_data['weight'] * join_index_data[col])/100
        useful_col.append(f"index_{col}")
        
    df = join_index_data.groupby(['index_code','index_name']).sum()[useful_col].reset_index(level='index_name')
    df.index.name = None

    # ä¿å­˜æŒ‡æ•°æ•°æ®ï¼ˆCSV æ ¼å¼ï¼‰ï¼Œä¿®å¤: å»é™¤æ–‡ä»¶åä¸­çš„å†’å·å¹¶åœ¨æƒé™å—é™æ—¶å›é€€è·¯å¾„
    safe_time_str = time_str.replace(':', '')
    index_dir = PATHS.index_data_path
    os.makedirs(index_dir, exist_ok=True)
    index_csv_path = os.path.join(index_dir, f'{safe_time_str}.csv')
    try:
        df.to_csv(index_csv_path, index=True)
        manage_index_files(index_dir, max_files=5)
        print(f"ğŸ“Š æŒ‡æ•°æ•°æ®å·²æ›´æ–°: {time_str} -> {index_csv_path}")
    except PermissionError:
        # å›é€€åˆ°ä¿å­˜ç›®å½•ä¸‹çš„ index_data å­ç›®å½•
        fallback_dir = os.path.join(save_path, 'index_data')
        os.makedirs(fallback_dir, exist_ok=True)
        index_csv_path = os.path.join(fallback_dir, f'{safe_time_str}.csv')
        df.to_csv(index_csv_path, index=True)
        manage_index_files(fallback_dir, max_files=5)
        print(f"ğŸ“Š æŒ‡æ•°æ•°æ®å·²æ›´æ–°(å›é€€): {time_str} -> {index_csv_path}")
    
    # ========== æ–°å¢ï¼šä¿å­˜ JSON æ ¼å¼ï¼ˆä¸»è¦æ ¼å¼ï¼‰ ==========
    file_base_name = f"test_{time_str.replace(':', '')}_idx{current_trading_index}"
    
    # å‡†å¤‡ JSON æ•°æ®ï¼ˆé‡ç½®ç´¢å¼•ï¼Œå°†è‚¡ç¥¨ä»£ç ä½œä¸ºå­—æ®µï¼‰
    json_data = final_data.reset_index()
    json_data.rename(columns={'index': 'code'}, inplace=True)
    
    # é‡å‘½ååˆ—ä»¥åŒ¹é…å‰ç«¯æœŸæœ›ï¼ˆç®€åŒ–å­—æ®µåï¼‰
    json_data.rename(columns={
        'rolling1_z_score': 'r1_z',
        'rolling5_z_score': 'r5_z',
        'rolling10_z_score': 'r10_z',
        'rolling30_z_score': 'r30_z',
        'rolling_full_z_score': 'rolling_full'
    }, inplace=True)
    
    # ä¿å­˜ä¸º JSONï¼ˆorient='records' ç”Ÿæˆæ•°ç»„æ ¼å¼ï¼‰ï¼Œæƒé™å›é€€åˆ° /tmp
    try:
        os.makedirs(save_path, exist_ok=True)
        json_target_dir = save_path
    except PermissionError:
        json_target_dir = os.path.join('/tmp', 'live_monitor', 'test_result')
        os.makedirs(json_target_dir, exist_ok=True)
    
    json_file_path = os.path.join(json_target_dir, f"{file_base_name}.json")
    json_data.to_json(
        json_file_path,
        orient='records',
        force_ascii=False,
        double_precision=2  # ä¿ç•™2ä½å°æ•°ï¼Œå‡å°æ–‡ä»¶å¤§å°
    )
    print(f"ğŸ’¾ JSONæ•°æ®å·²ä¿å­˜: {json_file_path} ({len(json_data)} æ¡è®°å½•)")
    
    # å¯é€‰ï¼šä¿ç•™ CSV ä½œä¸ºå¤‡ä»½ï¼ˆè°ƒè¯•ç”¨ï¼‰
    # csv_file_path = os.path.join(save_path, f"{file_base_name}.csv")
    # final_data.to_csv(csv_file_path)
    # print(f"ğŸ’¾ CSVæ•°æ®å·²ä¿å­˜: {file_base_name}.csv")
    # ======================================================
    
    # ç®¡ç†æ–‡ä»¶æ•°é‡ï¼ˆéœ€è¦åŒæ—¶ç®¡ç† JSON å’Œ CSVï¼‰
    manage_result_files(json_target_dir)

def wait_for_minute_start() -> None:
    """ç­‰å¾…åˆ°ä¸‹ä¸€åˆ†é’Ÿçš„å¼€å§‹"""
    now = datetime.now()
    if now.second != 0 or now.microsecond != 0:
        seconds_to_wait = CONFIG.MINUTE_SECONDS - now.second
        if now.microsecond > 0:
            seconds_to_wait -= 1
        time.sleep(seconds_to_wait + (1 - now.microsecond / 1_000_000 if now.microsecond > 0 else 0))

def print_startup_info(api_url: str, data_path: str, save_path: str) -> None:
    """æ‰“å°ç¨‹åºå¯åŠ¨ä¿¡æ¯"""
    print_trading_map_info()
    print(f"ğŸš€ è‚¡ç¥¨åˆ†æç¨‹åºå¯åŠ¨")
    print(f"ğŸ“¡ æ•°æ®æº: {api_url}")
    print(f"ğŸ“‚ ç»Ÿè®¡æ•°æ®: {data_path}")
    print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {save_path}")
    print(f"â±ï¸ æ›´æ–°é—´éš”: {CONFIG.UPDATE_INTERVAL} ç§’")
    print(f"ğŸ”„ ç¨‹åºè¿è¡Œè§„åˆ™:")
    print(f"   - äº¤æ˜“æ—¶é—´å†…æ­£å¸¸è¿è¡Œå¹¶å¤„ç†æ•°æ®")
    print(f"   - éäº¤æ˜“æ—¶é—´è‡ªåŠ¨ç¡çœ ç­‰å¾…")
    print(f"   - 15:00æ”¶ç›˜åè‡ªåŠ¨é€€å‡ºç¨‹åº")
    print(f"   - æ”¶ç›˜åå¯åŠ¨å°†ç­‰å¾…æ˜æ—¥9:30å¼€ç›˜")
    print(f"âŒ æŒ‰ Ctrl+C å¯å®‰å…¨åœæ­¢ç¨‹åº")
    print("=" * 60)

def _print_exit_message(save_path: str) -> None:
    """æ‰“å°ç¨‹åºé€€å‡ºä¿¡æ¯"""
    print(f"ğŸ”š äº¤æ˜“æ—¶é—´ç»“æŸï¼Œç¨‹åºæ­£å¸¸é€€å‡º")
    print(f"ğŸ“Š ç¨‹åºè¿è¡Œç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {save_path}")
    print(f"ğŸ‘‹ æ˜æ—¥äº¤æ˜“æ—¶é—´è§ï¼")

def _transform_market_data(fetched_data: Dict[str, Any]) -> Dict[str, List]:
    """è½¬æ¢å¸‚åœºæ•°æ®æ ¼å¼"""
    return {
        item["Symbol"]: [
            item['StockName'],      # 0:è‚¡ç¥¨åç§°
            item["Latest"],         # 1:æœ€æ–°ä»·
            item["ChangePercent"],  # 2:æ¶¨è·Œå¹…
            item["TradingVolume"]   # 3:äº¤æ˜“é‡
        ]
        for item in fetched_data["data"]
    }

# def _process_market_data(fetched_data: Dict[str, Any], time_str: str, 
#                         rolling: Dict[str, Dict], start_time: str,
#                         statistics_data: Dict, save_path: str, index_weight_data: pd.DataFrame) -> str:
#     """å¤„ç†å¸‚åœºæ•°æ®çš„ä¸»è¦é€»è¾‘"""
#     current_trading_index = time_to_trading_index(time_str)
    
#     # è®¾ç½®èµ·å§‹æ—¶é—´
#     if not start_time:
#         start_time = time_str
#         print(f"ğŸ¯ è®¾ç½®èµ·å§‹äº¤æ˜“æ—¶é—´: {start_time} (ç´¢å¼•: {current_trading_index})")
    
#     # è½¬æ¢æ•°æ®æ ¼å¼
#     transformed_data = _transform_market_data(fetched_data)
    
#     # åˆ›å»ºæ•°æ®ç»“æ„å¹¶åŠ å…¥rollingå­—å…¸
#     result_data = create_data_structure(transformed_data, rolling, time_str, start_time)
#     rolling[time_str] = result_data
    
#     print(f"âœ“ å½“å‰å¤„ç†æ—¶é—´: {time_str} (äº¤æ˜“ç´¢å¼•: {current_trading_index})")
#     print(f"ğŸ“Š rolling å­—å…¸çŠ¶æ€: {len(rolling)} ä¸ªæ—¶é—´ç‚¹ {sorted(rolling.keys())}")
    

#     # ç®¡ç†rollingå­—å…¸é•¿åº¦
#     if len(rolling) > CONFIG.MAX_ROLLING_LENGTH:
#         oldest_time = min(rolling.keys())
#         del rolling[oldest_time]
#         start_time = min(rolling.keys())
#         print(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸæ•°æ®: {oldest_time}")
    
#     # å¤„ç†æ•°æ®å¹¶ä¿å­˜
#     process_data_and_save(rolling, time_str, statistics_data, save_path, index_weight_data)
    
#     return start_time

import os
import pandas as pd
import numpy as np
from typing import Dict, Any

def new_create_data_structure(transformed_df: pd.DataFrame, 
                              rolling: Dict[str, Dict[str, Dict[str, Any]]],
                              time_str: str,
                              start_time: str) -> Dict[str, Dict[str, Any]]:
    """
    å°† DataFrame è½¬æ¢ä¸º rolling å­—å…¸éœ€è¦çš„æ ¼å¼ï¼ˆä¿è¯æ¯ä¸ª symbol å¯¹åº” dictï¼Œæ‰å¹³åŒ– listï¼‰ã€‚
    
    å‚æ•°:
        transformed_df: è½¬æ¢åçš„è¡Œæƒ… DataFrameï¼Œè‡³å°‘åŒ…å« 'symbol', 'name', 'price', 'change', 'volume'
        rolling: å½“å‰ rolling å­—å…¸ï¼Œå¯ç”¨äºå†å²æ•°æ®å¼•ç”¨
        time_str: å½“å‰æ—¶é—´ç‚¹
        start_time: èµ·å§‹æ—¶é—´
        
    è¿”å›:
        result_data: dictï¼Œç»“æ„ {symbol: {å­—æ®µ: å€¼, ...}}
    """
    
    def flatten_value(val):
        """é€’å½’æ‰å¹³åŒ– listï¼Œå¹¶è½¬æˆ float/int/string"""
        if isinstance(val, list) and len(val) > 0:
            return flatten_value(val[0])
        elif hasattr(val, 'item'):  # np.float64 / np.int64
            return val.item()
        elif isinstance(val, (float, int)):
            return val
        elif val is None or isinstance(val, pd._libs.missing.NAType):
            return 0
        else:
            return val  # å­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»å‹
    
    result_data = {}
    
    for _, row in transformed_df.iterrows():
        symbol = row['symbol']
        attr = {}
        for col in transformed_df.columns:
            if col == 'symbol':
                continue
            val = row[col]
            attr[col] = flatten_value(val)
        result_data[symbol] = attr
    
    return result_data


def _process_market_data(fetched_data: Dict[str, Any], time_str: str,  
                        rolling: Dict[str, Dict[str, Dict[str, Any]]],
                        start_time: str,
                        statistics_data: Dict,
                        save_path: str,
                        index_weight_data: pd.DataFrame) -> str:
    """å¤„ç†å¸‚åœºæ•°æ®ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–/åˆ†æç”¨ rolling_df"""
    
    current_trading_index = time_to_trading_index(time_str)

    if not start_time:
        start_time = time_str
        print(f"ğŸ¯ è®¾ç½®èµ·å§‹äº¤æ˜“æ—¶é—´: {start_time} (ç´¢å¼•: {current_trading_index})")

    # --- è½¬æ¢æ•°æ®æ ¼å¼ ---
    transformed_data = _transform_market_data(fetched_data)

    # --- è½¬æ¢ transformed_data ä¸º DataFrame ---
    if isinstance(transformed_data, dict):
        # dict -> DataFrameï¼Œkey æ˜¯ symbol
        transformed_df = pd.DataFrame.from_dict(transformed_data, orient='index').reset_index()
        transformed_df.rename(columns={'index': 'symbol'}, inplace=True)
    else:
        # å·²ç»æ˜¯ DataFrame
        transformed_df = transformed_data

    # --- ä¿å­˜ CSV ---
    debug_dir = os.path.join(save_path, "debug")
    os.makedirs(debug_dir, exist_ok=True)
    transformed_path = os.path.join(debug_dir, f"transformed_data_{time_str.replace(':', '-')}.csv")
    transformed_df.to_csv(transformed_path, index=False)
    print(f"ğŸ§¾ transformed_data å·²ä¿å­˜åˆ°: {transformed_path}")

    # --- åˆ›å»º rolling dict ---
    result_data = new_create_data_structure(transformed_data, rolling, time_str, start_time)
    rolling[time_str] = result_data

    # --- ç®¡ç† rolling dict é•¿åº¦ ---
    if len(rolling) > CONFIG.MAX_ROLLING_LENGTH:
        oldest_time = min(rolling.keys())
        del rolling[oldest_time]
        start_time = min(rolling.keys())
        print(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸæ•°æ®: {oldest_time}")

    # --- ç”Ÿæˆ rolling_df ç”¨äºå¯è§†åŒ–å’Œ CSV ---
    rolling_rows = []
    for t, data_dict in rolling.items():
        for symbol, attr in data_dict.items():
            row = {"time": t, "symbol": symbol}
            row.update(attr)
            rolling_rows.append(row)
    rolling_df = pd.DataFrame(rolling_rows)

    rolling_csv_path = os.path.join(debug_dir, "rolling_history.csv")
    rolling_df.to_csv(rolling_csv_path, index=False)
    print(f"ğŸ“ˆ rolling å†å²æ•°æ®å·²ä¿å­˜: {rolling_csv_path}")

    # --- ç¤ºä¾‹ z-score è®¡ç®—ï¼ˆå¯é€‰ï¼‰ ---
    # æŒ‰ symbol åˆ†ç»„ï¼Œè®¡ç®—è¿‡å» N ä¸ªæ—¶é—´ç‚¹çš„ price z-score
    N = 20
    rolling_df["price_mean"] = rolling_df.groupby("symbol")["price"].transform(lambda x: x.rolling(N, min_periods=1).mean())
    rolling_df["price_std"] = rolling_df.groupby("symbol")["price"].transform(lambda x: x.rolling(N, min_periods=1).std())
    rolling_df["price_zscore"] = (rolling_df["price"] - rolling_df["price_mean"]) / rolling_df["price_std"]

    # --- è°ƒç”¨åç»­å¤„ç†é€»è¾‘ ---
    process_data_and_save(rolling, time_str, statistics_data, save_path, index_weight_data)

    print(f"âœ“ å½“å‰å¤„ç†æ—¶é—´: {time_str} (äº¤æ˜“ç´¢å¼•: {current_trading_index})")
    print(f"ğŸ“Š rolling å­—å…¸çŠ¶æ€: {len(rolling)} ä¸ªæ—¶é—´ç‚¹ {sorted(rolling.keys())}")

    return start_time





def test_amz_data():
    import AmazingData as ad 
    import dotenv
    import os
    dotenv.load_dotenv()
    username = os.getenv("AMZ_USERNAME")
    password = os.getenv("AMZ_PWD")
    host = os.getenv("AMZ_HOST")
    port = int(os.getenv("AMZ_PORT"))
    ad.login(username, password,host,port) 
    base_data_object = ad.BaseData() 
    code_list = base_data_object.get_code_list(security_type=' EXTRA_INDEX_A ') 

    # å®æ—¶è®¢é˜… 
    sub_data = ad.SubscribeData() 
    @sub_data.register(code_list=code_list, period=ad.constant.Period.snapshot.value) 
    def onSnapshot(index: Union[ad.constant.Snapshot, ad.constant.SnapshotIndex], period):     
        print(period, data)  
    
    sub_data.run()  
    

import os
from minio import Minio
from datetime import datetime
from minio.error import S3Error

MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY")
MINIO_BUCKET = "live-data"

def download_latest_time_data(prefix: str = "time_data_", local_dir: str = "/app/statistic_data"):
    """
    ä» MinIO ä¸‹è½½æœ€æ–°çš„ç»Ÿè®¡æ•°æ®æ–‡ä»¶ (ä¾‹å¦‚ time_data_20251020.parquet)ï¼Œ
    å¹¶é‡å‘½åä¸ºå¸¦ä¸­åˆ’çº¿æ—¥æœŸçš„æ ¼å¼ (ä¾‹å¦‚ time_data_2025-10-20.parquet)ã€‚

    å‚æ•°:
        prefix (str): æ–‡ä»¶åå‰ç¼€ï¼ˆé»˜è®¤ 'time_data_'ï¼‰
        local_dir (str): æœ¬åœ°ä¸‹è½½ç›®å½•ï¼ˆé»˜è®¤ '/app/statistic_data'ï¼‰

    è¿”å›:
        str: ä¸‹è½½åˆ°æœ¬åœ°çš„å®Œæ•´æ–‡ä»¶è·¯å¾„
    """
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = Minio(
            MINIO_ENDPOINT,
            access_key=MINIO_ACCESS_KEY,
            secret_key=MINIO_SECRET_KEY,
            secure=False
        )

        if not client.bucket_exists(MINIO_BUCKET):
            raise ValueError(f"âŒ æ¡¶ä¸å­˜åœ¨: {MINIO_BUCKET}")

        # è·å–æ¡¶ä¸­æ‰€æœ‰ç¬¦åˆå‘½åçš„å¯¹è±¡
        objects = client.list_objects(MINIO_BUCKET, recursive=True)
        time_files = [
            obj.object_name for obj in objects
            if obj.object_name.startswith(prefix) and obj.object_name.endswith(".parquet")
        ]

        if not time_files:
            raise FileNotFoundError(f"âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶: {prefix}*.parquet")

        # æå–æ—¥æœŸå¹¶æ‰¾åˆ°æœ€æ–°æ–‡ä»¶
        def extract_date(name):
            try:
                date_str = name.replace(prefix, "").replace(".parquet", "")
                return datetime.strptime(date_str, "%Y%m%d")
            except Exception:
                return datetime.min

        latest_file = max(time_files, key=extract_date)
        latest_date = extract_date(latest_file).strftime("%Y-%m-%d")  # è½¬æˆå¸¦ä¸­åˆ’çº¿æ—¥æœŸ

        # æ„é€ æœ¬åœ°è·¯å¾„
        os.makedirs(local_dir, exist_ok=True)
        new_filename = f"{prefix}{latest_date}.parquet"
        local_path = os.path.join(local_dir, new_filename)

        # ä¸‹è½½å¹¶é‡å‘½å
        temp_path = os.path.join(local_dir, os.path.basename(latest_file))
        client.fget_object(MINIO_BUCKET, latest_file, temp_path)

        # é‡å‘½åï¼ˆè¦†ç›–æ—§æ–‡ä»¶ï¼‰
        if os.path.exists(local_path):
            os.remove(local_path)
        os.rename(temp_path, local_path)

        print(f"âœ… æœ€æ–°æ•°æ®æ–‡ä»¶å·²ä¸‹è½½å¹¶é‡å‘½å: {latest_file} â†’ {new_filename}")
        return local_path

    except S3Error as e:
        print(f"âŒ MinIO é”™è¯¯: {e}")
        raise
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise



def main(url: str = None, interval_seconds: int = None, previous_path: str = None, test_mode: bool = False) -> None:
    
    
    download_latest_time_data()

    """ä¸»å‡½æ•°ï¼šè‚¡ç¥¨åˆ†æç¨‹åºå…¥å£"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    api_url = url or API_URL
    data_path = previous_path or PATHS.previous_data_path
    # æµ‹è¯•æ¨¡å¼ä½¿ç”¨ç‹¬ç«‹çš„ä¿å­˜è·¯å¾„
    save_path = PATHS.test_save_data_path if test_mode else PATHS.save_data_path

    #test_amz_data()
    # æµ‹è¯•æ¨¡å¼æç¤º
    if test_mode:
        print("ğŸ§ª =============== æµ‹è¯•æ¨¡å¼å¯åŠ¨ ===============")
        print("âš ï¸ æ³¨æ„ï¼šæµ‹è¯•æ¨¡å¼å°†å¿½ç•¥äº¤æ˜“æ—¶é—´é™åˆ¶")
        print(f"ğŸ“ æµ‹è¯•æ•°æ®å°†ä¿å­˜åˆ°: {save_path}")
        print("âœ… ä¸ä¼šå½±å“æ­£å¼ç»“æœæ–‡ä»¶")
        print("ğŸ§ª =======================================")
        # ç¡®ä¿æµ‹è¯•ç›®å½•å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
    
    # åˆå§‹åŒ–
    start_time = ''
    index_weight_data = read_index_weight_data(PATHS.index_weight_data_path)
    
    # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬åŠ è½½ç»Ÿè®¡æ•°æ®ï¼ˆDataFrameæ ¼å¼ï¼Œæ€§èƒ½æå‡10-16å€ï¼‰
    print("ğŸ“Š åŠ è½½é¢„å¤„ç†ç»Ÿè®¡æ•°æ®...")
    statistics_data = read_previous_data_optimized(data_path)
    print(f"âœ… ç»Ÿè®¡æ•°æ®åŠ è½½å®Œæˆ: {statistics_data.shape[0]} è¡Œæ•°æ®")
    
    rolling: Dict[str, Dict[str, List]] = {}
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print_startup_info(api_url, data_path, save_path)
    
    # æ£€æŸ¥å¯åŠ¨æ—¶çš„æ—¶é—´çŠ¶æ€ - ä¼ å…¥æµ‹è¯•æ¨¡å¼å‚æ•°
    status, wait_seconds, next_session, message = get_time_status_and_sleep(test_mode)
    if status == 'exit' and not test_mode:  # æµ‹è¯•æ¨¡å¼ä¸‹ä¸é€€å‡º
        hours, minutes = format_time_duration(wait_seconds)[:2]
        print(f"\nğŸ“¢ å½“å‰ä¸ºæ”¶ç›˜åæ—¶é—´ï¼Œç¨‹åºå°†ç­‰å¾…æ˜æ—¥å¼€ç›˜")
        print(f"ğŸ’¤ é¢„è®¡ç­‰å¾…æ—¶é—´: {hours}å°æ—¶{minutes}åˆ†é’Ÿ")
        print(f"â° å°†åœ¨ {(datetime.now() + timedelta(seconds=wait_seconds)).strftime('%Y-%m-%d %H:%M:%S')} å¼€å§‹è¿è¡Œ")
        print(f"ğŸ”„ ç¨‹åºç°åœ¨è¿›å…¥ç¡çœ æ¨¡å¼...")
        # å®é™…è¿›å…¥ç¡çœ ç­‰å¾…
        handle_sleep(wait_seconds, next_session)

    try:
        while True:
            # æ£€æŸ¥å½“å‰æ—¶é—´çŠ¶æ€ - ä¼ å…¥æµ‹è¯•æ¨¡å¼å‚æ•°
            status, wait_seconds, next_session, message = get_time_status_and_sleep(test_mode)
            print(message)
            
            if status == 'exit' and not test_mode:  # æµ‹è¯•æ¨¡å¼ä¸‹ä¸é€€å‡º
                _print_exit_message(save_path)
                break
                
            elif status == 'wait' and not test_mode:  # æµ‹è¯•æ¨¡å¼ä¸‹ä¸ç­‰å¾…
                handle_sleep(wait_seconds, next_session)
                continue
            
            # ç­‰å¾…åˆ°åˆ†é’Ÿå¼€å§‹
            wait_for_minute_start()
            
            # è·å–å½“å‰æ ‡å‡†äº¤æ˜“æ—¶é—´
            current_time_str = datetime.now().strftime('%H:%M:00')
            
            # éªŒè¯äº¤æ˜“æ—¶é—´ (æµ‹è¯•æ¨¡å¼ä¸‹è·³è¿‡éªŒè¯)
            if not test_mode and not is_trading_time(current_time_str):
                print(f"âš ï¸ å½“å‰æ—¶é—´ {current_time_str} ä¸åœ¨äº¤æ˜“æ—¶é—´æ˜ å°„èŒƒå›´å†…ï¼Œç­‰å¾…30ç§’åé‡æ–°æ£€æŸ¥...")
                time.sleep(30)
                continue
                
            print(f"\n--- æ­£åœ¨è·å–æ•°æ® (äº¤æ˜“æ—¶é—´: {current_time_str}, ç´¢å¼•: {time_to_trading_index(current_time_str) if not test_mode else 'TEST'}) ---")
            
            # è·å–æ•°æ®ï¼ˆåœ¨çº¿æˆ–æµ‹è¯•æ¨¡å¼ï¼‰
            fetched_data = fetch_minute_data(api_url)
            if fetched_data:
                # å¤„ç†æ•°æ®
                start_time = _process_market_data(
                    fetched_data, current_time_str, rolling, 
                    start_time, statistics_data, save_path, index_weight_data
                )
                
                print(f"â° ç­‰å¾…ä¸‹ä¸€åˆ†é’Ÿ...")
                
                # æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œåªè¿è¡Œä¸€æ¬¡å°±é€€å‡º
                if test_mode:
                    print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå·²å®Œæˆä¸€æ¬¡æ•°æ®å¤„ç†ï¼Œç¨‹åºé€€å‡º")
                    break
        
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç¨‹åºæ­£åœ¨å®‰å…¨é€€å‡º...")
        print(f"ğŸ“Š æœ€åå¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ‘‹ ç¨‹åºå·²å®‰å…¨åœæ­¢")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        print(f"ğŸ“Š é”™è¯¯å‘ç”Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ”§ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€æ•°æ®æºæˆ–é…ç½®æ–‡ä»¶")
        raise




if __name__ == "__main__":
    import sys
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ä¸­æ˜¯å¦æœ‰test_mode
    test_mode = "--test" in sys.argv or "--test-mode" in sys.argv
    main(test_mode=test_mode)



