import pandas as pd
import os
import glob
from collections import defaultdict
from tqdm import tqdm
import numpy as np
import warnings
from datetime import datetime
import dotenv
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import pickle
from functools import partial
import pyarrow.parquet as pq
import gzip
import pandas as pd
from minio import Minio
import os
from datetime import datetime
import pandas as pd
from minio import Minio
import os
from minio import Minio
from dotenv import load_dotenv
from data_fetcher import get_server_data

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()  # é»˜è®¤ä¼šåœ¨å½“å‰å·¥ä½œç›®å½•æŸ¥æ‰¾ .env

# è¯»å–ç¯å¢ƒå˜é‡
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY")
MINIO_BUCKET = "live-data"

# =============== åˆå§‹åŒ–è®¾ç½® ===============
# å¿½ç•¥æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings("ignore")
# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()
ON_SERVER = os.getenv("ON_SERVER", "false") == "true"

# =============== åŸºç¡€é…ç½® ===============
# è®¾ç½®è·¯å¾„ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼Œä¸ timely_data.py ä¿æŒä¸€è‡´ï¼‰
DATA_ROOT = os.getenv('DATA_ROOT', os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(DATA_ROOT, "app/data")
OUTPUT_DIR = os.path.join(DATA_ROOT, "statistic_data")  # ç°åœ¨ä¼šæŒ‡å‘ /app/statistic_data âœ…

# æ—¶é—´è®¾ç½®
TODAY_DATE = datetime.now().strftime('%Y-%m-%d')
if ON_SERVER:
    TARGET_DATE = TODAY_DATE
else:
    TARGET_DATE = "2025-07-01"  # è®¾ç½®å›ºå®šæ—¥æœŸç”¨äºæµ‹è¯•

# æ•°æ®å¤„ç†å‚æ•°
WINDOW_LENGTH_LIST = [1, 5, 10, 30]  # æ»šåŠ¨çª—å£é•¿åº¦
INCLUDE_FULL_ROLLING = True  # æ˜¯å¦åŒ…å«å…¨é‡æ»šåŠ¨è®¡ç®—
DATE_INTERVAL = int(os.getenv("DATE_INTERVAL", 15))  # æ—¥æœŸé—´éš”æ”¹ä¸º30å¤©
BATCH_SIZE = 100  # æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
NUM_PROCESSES = int(os.getenv("NUM_PROCESSES", max(1, mp.cpu_count() - 1)))  # è¿›ç¨‹æ•°ï¼ˆç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿï¼‰
KEEP_FILE_COUNT = 2  # ä¿ç•™çš„å†å²æ–‡ä»¶æ•°é‡


"""
ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†è„šæœ¬
ç›´æ¥å¤„ç†åŸå§‹æ•°æ®åˆ°æœ€ç»ˆçš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œä¸ä¿å­˜ä¸­é—´æ–‡ä»¶
æœ€ç»ˆè¾“å‡ºï¼šstatistic_data/time_data_{target_date}.json
"""

def get_stock_data() -> pd.DataFrame:
    """
    è·å–è‚¡ç¥¨æ•°æ®çš„ç»Ÿä¸€æ¥å£
    æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æ•°æ®æº
    """
    try:
        
        print("ä½¿ç”¨æœåŠ¡å™¨æ•°æ®æº...")
        df = get_server_data()
        df['vol'] = df['vol']
        print(f"æˆåŠŸè·å–æœåŠ¡å™¨æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
        return df
    except ImportError:
        print("ä½¿ç”¨æœ¬åœ°æµ‹è¯•æ•°æ®æº...")
        current_dir = os.path.dirname(os.path.abspath(__file__))
        csv_path = os.path.join(current_dir, "data", "stock_minute_data_test.csv")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        df = pd.read_csv(csv_path)
        df['vol'] = df['vol']
        print(f"æˆåŠŸè¯»å–æµ‹è¯•æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
        return df
    except Exception as e:
        print(f"æ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise

def preprocess_stock_minute_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ•°æ®é¢„å¤„ç† - ç°åœ¨æ¥å—DataFrameè€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„"""
    print("æ­£åœ¨é¢„å¤„ç†è‚¡ç¥¨åˆ†é’Ÿæ•°æ®...")
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨categoricalç±»å‹å‡å°‘å†…å­˜ä½¿ç”¨
    df['ts_code'] = df['ts_code'].astype('category')
    df['trade_time'] = pd.to_datetime(df['trade_time'])
    df['trade_date'] = df['trade_time'].dt.date
    df['time_only'] = df['trade_time'].dt.time
    df = df.sort_values(['trade_date', 'ts_code', 'trade_time'])
    
    return df

def process_single_group(group_data):
    """å¤„ç†å•ä¸ªè‚¡ç¥¨ç»„çš„æ•°æ®"""
    result = {}
    
    # è®¡ç®—æ‰€æœ‰çª—å£çš„æ»šåŠ¨æ±‚å’Œï¼ˆæ”¹ä¸ºsumï¼‰
    for window_length in WINDOW_LENGTH_LIST:
        rolling_vol = group_data['vol'].rolling(
            window=window_length,
            min_periods=1
        ).sum().values  # æ”¹ä¸ºsum()
        
        rolling_dict = {
            time.strftime("%H:%M:%S"): val
            for time, val in zip(group_data['time_only'], rolling_vol)
        }
        
        result[f'rolling{window_length}'] = rolling_dict
    
    # ä¿®æ”¹rolling_fullçš„è®¡ç®—æ–¹å¼ - æ”¹ä¸ºç´¯ç§¯æ±‚å’Œ
    if INCLUDE_FULL_ROLLING:
        # è®¡ç®—ä»å¼€ç›˜åˆ°æ¯ä¸ªæ—¶é—´ç‚¹çš„ç´¯ç§¯æ±‚å’Œ
        cumsum = np.cumsum(group_data['vol'].values)
        
        # è®¡ç®—ä»å¼€ç›˜åˆ°æ¯ä¸ªæ—¶é—´ç‚¹çš„æ ‡å‡†å·®ï¼ˆåŸºäºåŸå§‹å€¼ï¼‰
        indices = np.arange(1, len(group_data) + 1)
        vol_values = group_data['vol'].values
        
        # è®¡ç®—ç´¯ç§¯å‡å€¼ç”¨äºæ ‡å‡†å·®è®¡ç®—
        cumulative_means = cumsum / indices
        
        # è®¡ç®—æ ‡å‡†å·®
        squared_diff_cumsum = np.cumsum(np.square(vol_values - cumulative_means))
        stds = np.sqrt(squared_diff_cumsum / indices)
        stds = np.where(stds == 0, 1e-8, stds)  # é¿å…é™¤ä»¥0
        
        # åˆ›å»ºç´¯ç§¯æ±‚å’Œå’Œæ ‡å‡†å·®çš„å­—å…¸
        sum_dict = {
            time.strftime("%H:%M:%S"): float(cumsum_val)
            for time, cumsum_val in zip(group_data['time_only'], cumsum)
        }
        
        std_dict = {
            time.strftime("%H:%M:%S"): float(std)
            for time, std in zip(group_data['time_only'], stds)
        }
        
        # ä¿å­˜ç´¯ç§¯æ±‚å’Œå’Œæ ‡å‡†å·®
        result['rolling_full'] = {
            'mean': sum_dict,  # è¿™é‡Œå­˜å‚¨çš„å®é™…æ˜¯ç´¯ç§¯æ±‚å’Œ
            'std': std_dict
        }
    
    return result

def process_stock_chunk(stock_chunk, window_lengths, include_full_rolling):
    """
    å¤„ç†ä¸€æ‰¹è‚¡ç¥¨çš„æ•°æ® - ä¼˜åŒ–åçš„æ‰¹å¤„ç†å‡½æ•°
    Args:
        stock_chunk: [(date, stock_code, group_data), ...]
        window_lengths: çª—å£é•¿åº¦åˆ—è¡¨
        include_full_rolling: æ˜¯å¦åŒ…å«å…¨é‡æ»šåŠ¨
    """
    results = {}
    
    for trade_date, ts_code, group_data in stock_chunk:
        try:
            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œè®¡ç®—æ»šåŠ¨æ•°æ®
            result = {}
            
            # æ‰¹é‡è®¡ç®—æ‰€æœ‰çª—å£çš„æ»šåŠ¨æ±‚å’Œï¼ˆæ”¹ä¸ºsumï¼‰
            for window_length in window_lengths:
                rolling_vol = group_data['vol'].rolling(
                    window=window_length,
                    min_periods=1
                ).sum()  # æ”¹ä¸ºsum()
                
                result[f'rolling{window_length}'] = {
                    time.strftime("%H:%M:%S"): val
                    for time, val in zip(group_data['time_only'], rolling_vol)
                }
            
            # ä¼˜åŒ–çš„rolling_fullè®¡ç®— - æ”¹ä¸ºç´¯ç§¯æ±‚å’Œ
            if include_full_rolling:
                vol_values = group_data['vol'].values
                indices = np.arange(1, len(vol_values) + 1)
                
                # è®¡ç®—ç´¯ç§¯æ±‚å’Œ
                cumsum = np.cumsum(vol_values)
                
                # è®¡ç®—ç´¯ç§¯å‡å€¼ç”¨äºæ ‡å‡†å·®è®¡ç®—
                cumulative_means = cumsum / indices
                
                # æ›´é«˜æ•ˆçš„æ ‡å‡†å·®è®¡ç®—
                cumsum_sq = np.cumsum(vol_values ** 2)
                stds = np.sqrt((cumsum_sq / indices) - (cumulative_means ** 2))
                stds = np.where(stds == 0, 1e-8, stds)
                
                result['rolling_full'] = {
                    'mean': {  # è¿™é‡Œå­˜å‚¨çš„å®é™…æ˜¯ç´¯ç§¯æ±‚å’Œ
                        time.strftime("%H:%M:%S"): float(cumsum_val)
                        for time, cumsum_val in zip(group_data['time_only'], cumsum)
                    },
                    'std': {
                        time.strftime("%H:%M:%S"): float(std)
                        for time, std in zip(group_data['time_only'], stds)
                    }
                }
            
            # å­˜å‚¨ç»“æœ
            if trade_date not in results:
                results[trade_date] = {}
            results[trade_date][ts_code] = result
            
        except Exception as e:
            print(f"å¤„ç†è‚¡ç¥¨ {ts_code} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    return results

def calculate_rolling_data_parallel_optimized(df: pd.DataFrame) -> dict:
    """ä¼˜åŒ–åçš„å¹¶è¡Œæ»šåŠ¨æ•°æ®è®¡ç®—"""
    print("æ­£åœ¨è®¡ç®—æ»šåŠ¨æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
    
    # é¢„å¤„ç†ï¼šæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œå‡å°‘åˆ†ç»„æ“ä½œ
    print("æ­£åœ¨åˆ†ç»„æ•°æ®...")
    groups = []
    for (trade_date, ts_code), group_data in df.groupby(['trade_date', 'ts_code']):
        if not group_data.empty:
            # é¢„æ’åºå¹¶é‡ç½®ç´¢å¼•ï¼Œå‡å°‘å­è¿›ç¨‹ä¸­çš„æ“ä½œ
            sorted_data = group_data.sort_values('trade_time').reset_index(drop=True)
            groups.append((str(trade_date), ts_code, sorted_data))
    
    # è®¡ç®—æœ€ä¼˜çš„æ‰¹å¤§å°
    optimal_chunk_size = max(1, len(groups) // (NUM_PROCESSES * 4))  # æ¯ä¸ªè¿›ç¨‹å¤„ç†4æ‰¹
    print(f"æ€»å…± {len(groups)} ä¸ªè‚¡ç¥¨ç»„ï¼Œæ¯æ‰¹ {optimal_chunk_size} ä¸ª")
    
    # å°†ä»»åŠ¡åˆ†æˆæ›´å¤§çš„å—
    chunks = []
    for i in range(0, len(groups), optimal_chunk_size):
        chunk = groups[i:i + optimal_chunk_size]
        chunks.append(chunk)
    
    print(f"åˆ†æˆ {len(chunks)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œå¤„ç†")
    
    # ä½¿ç”¨éƒ¨åˆ†å‡½æ•°æ¥ä¼ é€’é…ç½®å‚æ•°
    process_func = partial(
        process_stock_chunk,
        window_lengths=WINDOW_LENGTH_LIST,
        include_full_rolling=INCLUDE_FULL_ROLLING
    )
    
    # å¹¶è¡Œå¤„ç†
    final_data = {}
    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
        # æäº¤æ‰€æœ‰æ‰¹æ¬¡
        future_to_chunk = {
            executor.submit(process_func, chunk): chunk 
            for chunk in chunks
        }
        
        # æ”¶é›†ç»“æœ
        for future in tqdm(as_completed(future_to_chunk), 
                          total=len(future_to_chunk), 
                          desc="å¤„ç†è‚¡ç¥¨æ‰¹æ¬¡"):
            try:
                chunk_result = future.result()
                
                # åˆå¹¶ç»“æœ
                for date, stocks_data in chunk_result.items():
                    if date not in final_data:
                        final_data[date] = {}
                    final_data[date].update(stocks_data)
                    
            except Exception as e:
                print(f"å¤„ç†æ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                continue
    
    return final_data

def get_trading_stocks_for_date(data_dict: dict, target_date_str: str):
    """è·å–ç›®æ ‡æ—¥æœŸçš„äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨"""
    try:
        target_date = pd.to_datetime(target_date_str)
    except ValueError:
        print(f"Error: Invalid date format: {target_date_str}")
        return [], None
    
    # è·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸå¹¶æ’åº
    available_dates = sorted([pd.to_datetime(date) for date in data_dict.keys()])
    valid_dates = [date for date in available_dates if date <= target_date]
    
    if not valid_dates:
        print(f"Warning: No trading data found on or before {target_date_str}")
        return [], None
    
    # ä½¿ç”¨æœ€è¿‘çš„äº¤æ˜“æ—¥
    actual_date = max(valid_dates)
    actual_date_str = actual_date.strftime('%Y-%m-%d')
    
    if actual_date_str in data_dict:
        trading_stocks = list(data_dict[actual_date_str].keys())
        return trading_stocks, actual_date_str
    
    return [], None

def get_previous_n_trading_dates(data_dict: dict, target_date_str: str, stock_code: str, n: int = 3) -> list:
    """è·å–æŒ‡å®šè‚¡ç¥¨çš„å‰nä¸ªäº¤æ˜“æ—¥"""
    try:
        target_date = pd.to_datetime(target_date_str)
    except ValueError:
        return []
    
    # æ”¶é›†è¯¥è‚¡ç¥¨æœ‰æ•°æ®çš„æ‰€æœ‰æ—¥æœŸ
    valid_dates = []
    for date_str, stocks_data in data_dict.items():
        date_obj = pd.to_datetime(date_str)
        if date_obj <= target_date and stock_code in stocks_data:
            valid_dates.append(date_str)
    
    # æ’åºå¹¶å–å‰nä¸ª
    valid_dates.sort(key=pd.to_datetime, reverse=True)
    return valid_dates[:n]

def process_single_stock_stats(stock_code, dates_list, stock_data_subset, window_lengths):
    """å¤„ç†å•ä¸ªè‚¡ç¥¨çš„ç»Ÿè®¡æ•°æ®"""
    stock_stats = {}
    
    # å¤„ç†å¸¸è§„æ»šåŠ¨çª—å£
    for window_length in window_lengths:
        window_key = f'rolling{window_length}'
        daily_data_list = []
        
        for date_str in dates_list:
            if window_key in stock_data_subset.get(date_str, {}):
                daily_data_list.append(stock_data_subset[date_str][window_key])
        
        if daily_data_list:
            time_values = defaultdict(list)
            for daily_data in daily_data_list:
                for time, value in daily_data.items():
                    time_values[time].append(value)
            
            mean_dict = {
                time: round(float(np.mean(values)), 2)
                for time, values in time_values.items()
            }
            std_dict = {
                time: round(float(np.std(values)), 2)
                for time, values in time_values.items()
            }
            
            stock_stats[window_key] = {
                "mean": mean_dict,
                "std": std_dict
            }
        else:
            stock_stats[window_key] = {
                "mean": {},
                "std": {}
            }
    
    # å¤„ç†rolling_full
    if INCLUDE_FULL_ROLLING:
        daily_mean_list = []
        daily_values_dict = defaultdict(list)  # ç”¨äºå­˜å‚¨æ¯ä¸ªæ—¶é—´ç‚¹çš„åŸå§‹å€¼
        
        for date_str in dates_list:
            if 'rolling_full' in stock_data_subset.get(date_str, {}):
                data = stock_data_subset[date_str]['rolling_full']
                daily_mean_list.append(data['mean'])
                
                # æ”¶é›†æ¯ä¸ªæ—¶é—´ç‚¹çš„åŸå§‹å€¼
                for time, mean_val in data['mean'].items():
                    daily_values_dict[time].append(mean_val)
        
        if daily_mean_list:
            # å¤„ç†å‡å€¼
            mean_time_values = defaultdict(list)
            for daily_mean in daily_mean_list:
                for time, value in daily_mean.items():
                    mean_time_values[time].append(value)
            
            # è®¡ç®—æœ€ç»ˆçš„å‡å€¼å’Œæ ‡å‡†å·®
            final_mean_dict = {
                time: round(float(np.mean(values)), 2)
                for time, values in mean_time_values.items()
            }
            
            # ç›´æ¥å¯¹åŸå§‹å€¼è®¡ç®—æ ‡å‡†å·®
            final_std_dict = {
                time: round(float(np.std(values)), 2)
                for time, values in daily_values_dict.items()
            }
            
            stock_stats['rolling_full'] = {
                "mean": final_mean_dict,
                "std": final_std_dict
            }
        else:
            stock_stats['rolling_full'] = {
                "mean": {},
                "std": {}
            }
    
    return stock_stats

def process_statistics_batch(batch_data):
    """å¤„ç†ç»Ÿè®¡æ•°æ®æ‰¹æ¬¡"""
    batch_results = {}
    
    for stock_code, (dates_list, stock_data_subset) in batch_data.items():
        try:
            stock_stats = {}
            
            # å¤„ç†å¸¸è§„æ»šåŠ¨çª—å£ - å‘é‡åŒ–æ“ä½œ
            for window_length in WINDOW_LENGTH_LIST:
                window_key = f'rolling{window_length}'
                
                # æ”¶é›†æ‰€æœ‰æ—¥æœŸçš„æ•°æ®
                all_time_values = defaultdict(list)
                for date_str in dates_list:
                    if window_key in stock_data_subset.get(date_str, {}):
                        daily_data = stock_data_subset[date_str][window_key]
                        for time, value in daily_data.items():
                            all_time_values[time].append(value)
                
                if all_time_values:
                    # å‘é‡åŒ–è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
                    mean_dict = {}
                    std_dict = {}
                    
                    for time, values in all_time_values.items():
                        values_array = np.array(values)
                        mean_dict[time] = round(float(np.mean(values_array)), 2)
                        std_dict[time] = round(float(np.std(values_array)), 2)
                    
                    stock_stats[window_key] = {
                        "mean": mean_dict,
                        "std": std_dict
                    }
                else:
                    stock_stats[window_key] = {"mean": {}, "std": {}}
            
            # å¤„ç†rolling_full - åŒæ ·å‘é‡åŒ–
            if INCLUDE_FULL_ROLLING:
                all_mean_values = defaultdict(list)
                
                for date_str in dates_list:
                    if 'rolling_full' in stock_data_subset.get(date_str, {}):
                        data = stock_data_subset[date_str]['rolling_full']
                        for time, mean_val in data['mean'].items():
                            all_mean_values[time].append(mean_val)
                
                if all_mean_values:
                    final_mean_dict = {}
                    final_std_dict = {}
                    
                    for time, values in all_mean_values.items():
                        values_array = np.array(values)
                        final_mean_dict[time] = round(float(np.mean(values_array)), 2)
                        final_std_dict[time] = round(float(np.std(values_array)), 2)
                    
                    stock_stats['rolling_full'] = {
                        "mean": final_mean_dict,
                        "std": final_std_dict
                    }
                else:
                    stock_stats['rolling_full'] = {"mean": {}, "std": {}}
            
            batch_results[stock_code] = stock_stats
            
        except Exception as e:
            print(f"å¤„ç†è‚¡ç¥¨ {stock_code} ç»Ÿè®¡æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    return batch_results

def process_statistics_data_optimized(data_dict: dict, target_date: str, date_interval: int):
    """ä¼˜åŒ–åçš„ç»Ÿè®¡æ•°æ®å¤„ç†"""
    print("æ­£åœ¨å¤„ç†ç»Ÿè®¡æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
    
    # è·å–äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨
    stock_list, actual_date = get_trading_stocks_for_date(data_dict, target_date)
    print(f"æ‰¾åˆ° {len(stock_list)} åªè‚¡ç¥¨ï¼Œå®é™…æ—¥æœŸ: {actual_date}")
    
    if not stock_list:
        return {}
    
    # å¹¶è¡Œè·å–è‚¡ç¥¨æœ‰æ•ˆæ—¥æœŸ
    print("æ­£åœ¨è·å–è‚¡ç¥¨æœ‰æ•ˆäº¤æ˜“æ—¥æœŸ...")
    
    # ä½¿ç”¨éƒ¨åˆ†å‡½æ•°æ¥ä¼ é€’å‚æ•°
    get_dates_func = partial(
        get_stock_dates_batch,
        data_dict=data_dict,

        
        actual_date=actual_date,
        date_interval=date_interval
    )
    
    # å°†è‚¡ç¥¨åˆ—è¡¨åˆ†æ‰¹
    stock_chunks = [stock_list[i:i+100] for i in range(0, len(stock_list), 100)]
    
    stock_dates = {}
    with ThreadPoolExecutor(max_workers=min(4, len(stock_chunks))) as executor:
        future_to_chunk = {
            executor.submit(get_dates_func, chunk): chunk 
            for chunk in stock_chunks
        }
        
        for future in as_completed(future_to_chunk):
            batch_result = future.result()
            stock_dates.update(batch_result)
    
    print(f"è·å–åˆ° {len(stock_dates)} åªè‚¡ç¥¨çš„æœ‰æ•ˆæ—¥æœŸæ•°æ®")
    
    # é¢„å¤„ç†æ•°æ®
    processed_data = {}
    for stock_code, dates_list in stock_dates.items():
        processed_data[stock_code] = (
            dates_list,
            {
                date: {
                    k: v for k, v in data_dict[date][stock_code].items()
                    if k.startswith('rolling') or k == 'rolling_full'
                }
                for date in dates_list
                if date in data_dict and stock_code in data_dict[date]
            }
        )
    
    # ä¼˜åŒ–çš„æ‰¹å¤„ç†
    optimal_batch_size = max(10, len(processed_data) // (NUM_PROCESSES * 2))
    print(f"ä½¿ç”¨æ‰¹å¤§å°: {optimal_batch_size}")
    
    # å°†å¤„ç†æ•°æ®åˆ†æ‰¹
    data_items = list(processed_data.items())
    batches = []
    for i in range(0, len(data_items), optimal_batch_size):
        batch = dict(data_items[i:i + optimal_batch_size])
        batches.append(batch)
    
    # å¹¶è¡Œå¤„ç†ç»Ÿè®¡æ•°æ®
    stats_data = {}
    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
        future_to_batch = {
            executor.submit(process_statistics_batch, batch): batch 
            for batch in batches
        }
        
        for future in tqdm(as_completed(future_to_batch), 
                          total=len(future_to_batch), 
                          desc="å¤„ç†ç»Ÿè®¡æ•°æ®æ‰¹æ¬¡"):
            try:
                batch_result = future.result()
                stats_data.update(batch_result)
            except Exception as e:
                print(f"å¤„ç†ç»Ÿè®¡æ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                continue
    
    return stats_data

def get_stock_dates_batch(stocks_batch, data_dict, actual_date, date_interval):
    """æ‰¹å¤„ç†è·å–è‚¡ç¥¨æœ‰æ•ˆæ—¥æœŸ - æå–åˆ°æ¨¡å—çº§åˆ«ä»¥æ”¯æŒå¤šè¿›ç¨‹"""
    batch_result = {}
    for stock in stocks_batch:
        dates = get_previous_n_trading_dates(data_dict, actual_date, stock, date_interval)
        if dates:
            batch_result[stock] = dates
    return batch_result

def process_time_chunk(chunk_data):
    """å¤„ç†æ—¶é—´æ•°æ®å— - æå–åˆ°æ¨¡å—çº§åˆ«ä»¥æ”¯æŒå¤šè¿›ç¨‹"""
    time_data = {}
    
    for stock_code, stock_data in chunk_data.items():
        for window_type, window_data in stock_data.items():
            for metric_type, time_series_data in window_data.items():
                for time, value in time_series_data.items():
                    if time not in time_data:
                        time_data[time] = {}
                    if stock_code not in time_data[time]:
                        time_data[time][stock_code] = {}
                    if window_type not in time_data[time][stock_code]:
                        time_data[time][stock_code][window_type] = {}
                    
                    time_data[time][stock_code][window_type][metric_type] = value
    
    return time_data

def convert_to_time_format_parallel(stats_data: dict) -> dict:
    """å¹¶è¡Œè½¬æ¢ä¸ºæ—¶é—´åºåˆ—æ ¼å¼"""
    print("æ­£åœ¨è½¬æ¢ä¸ºæ—¶é—´åºåˆ—æ ¼å¼ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰...")
    
    # å°†è‚¡ç¥¨æ•°æ®åˆ†å—
    stock_items = list(stats_data.items())
    chunk_size = max(50, len(stock_items) // NUM_PROCESSES)
    
    chunks = []
    for i in range(0, len(stock_items), chunk_size):
        chunk = dict(stock_items[i:i + chunk_size])
        chunks.append(chunk)
    
    # å¹¶è¡Œå¤„ç†
    final_time_data = {}
    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
        future_to_chunk = {
            executor.submit(process_time_chunk, chunk): chunk 
            for chunk in chunks
        }
        
        for future in tqdm(as_completed(future_to_chunk), 
                          total=len(future_to_chunk), 
                          desc="è½¬æ¢æ—¶é—´æ ¼å¼"):
            try:
                chunk_result = future.result()
                
                # åˆå¹¶æ—¶é—´æ•°æ®
                for time, time_data in chunk_result.items():
                    if time not in final_time_data:
                        final_time_data[time] = {}
                    
                    for stock_code, stock_data in time_data.items():
                        if stock_code not in final_time_data[time]:
                            final_time_data[time][stock_code] = {}
                        
                        for window_type, window_data in stock_data.items():
                            final_time_data[time][stock_code][window_type] = window_data
                            
            except Exception as e:
                print(f"è½¬æ¢æ—¶é—´æ ¼å¼æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                continue
    
    return final_time_data


def upload_to_minio(file_path: str, object_name: str):
    """
    ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ° MinIO
    """
    client = Minio(
        MINIO_ENDPOINT,
        access_key=MINIO_ACCESS_KEY,
        secret_key=MINIO_SECRET_KEY,
        secure=False
    )

    if not client.bucket_exists(MINIO_BUCKET):
        client.make_bucket(MINIO_BUCKET)
    
    try:
        client.fput_object(MINIO_BUCKET, object_name, file_path)
        print(f"â˜ï¸ æ–‡ä»¶å·²ä¸Šä¼ è‡³ MinIO: {MINIO_BUCKET}/{object_name}")
    except Exception as e:
        print(f"âŒ ä¸Šä¼  MinIO å¤±è´¥: {e}")


def save_data_as_parquet(final_data: dict, output_path: str):
    """å°†æ•°æ®ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    rows = []
    for time_str, stocks_data in final_data.items():
        for stock_code, stock_data in stocks_data.items():
            for window_type, metrics in stock_data.items():
                rows.append({
                    'time': time_str,
                    'stock_code': stock_code,
                    'window_type': window_type,
                    'mean': metrics.get('mean', 0.0),
                    'std': metrics.get('std', 0.0)
                })
    
    df = pd.DataFrame(rows)
    
    # ä¼˜åŒ–æ•°æ®ç±»å‹
    df['time'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.time
    df['stock_code'] = df['stock_code'].astype('category')
    df['window_type'] = df['window_type'].astype('category')
    df['mean'] = df['mean'].astype('float32')
    df['std'] = df['std'].astype('float32')
    
    # ä½¿ç”¨å¿«é€Ÿå‹ç¼©ç®—æ³•
    parquet_path = output_path.replace('.json', '.parquet')
    df.to_parquet(
        parquet_path, 
        engine='pyarrow',
        compression='snappy',
        index=False,
        use_dictionary=True,
        compression_level=None
    )
    
    file_size = os.path.getsize(parquet_path) / (1024 * 1024)
    print(f"ğŸ’¾ Parquetæ–‡ä»¶ä¿å­˜è‡³: {parquet_path} ({file_size:.2f} MB)")

    # ç”Ÿæˆå¯¹è±¡åï¼Œæ¯”å¦‚ç”¨æ—¥æœŸæ—¶é—´åŒºåˆ†
    target_date = datetime.now().strftime("%Y%m%d")
    object_name = f"time_data_{target_date}.parquet"

    # ä¸Šä¼ åˆ° MinIO
    upload_to_minio(parquet_path, object_name)
    return parquet_path

def load_data_from_parquet(parquet_path: str) -> dict:
    """ä»Parquetæ–‡ä»¶è¯»å–æ•°æ®å¹¶è½¬æ¢å›åŸæ ¼å¼ï¼ˆè¶…å¿«ç‰ˆæœ¬ï¼‰"""
    import pyarrow.parquet as pq
    
    # è¯»å–æ•°æ®
    table = pq.read_table(parquet_path)
    df = table.to_pandas()
    
    # è½¬æ¢æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    df['time_str'] = df['time'].astype(str)
    
    # ä½¿ç”¨numpyæ•°ç»„åŠ é€Ÿï¼ˆé¿å…pandasçš„å¼€é”€ï¼‰
    times = df['time_str'].values
    stocks = df['stock_code'].values
    windows = df['window_type'].values
    means = df['mean'].values
    stds = df['std'].values
    
    # æ„å»ºç»“æœå­—å…¸ - å•æ¬¡éå† O(n)
    final_data = {}
    
    for i in range(len(df)):
        time_str = times[i]
        stock_code = stocks[i]
        window_type = windows[i]
        
        if time_str not in final_data:
            final_data[time_str] = {}
        
        if stock_code not in final_data[time_str]:
            final_data[time_str][stock_code] = {}
        
        final_data[time_str][stock_code][window_type] = {
            'mean': float(means[i]),
            'std': float(stds[i])
        }
    
    return final_data

def save_data_as_csv_gz(final_data: dict, output_path: str):
    """å°†æ•°æ®ä¿å­˜ä¸ºå‹ç¼©CSVæ ¼å¼"""
    rows = []
    for time_str, stocks_data in final_data.items():
        for stock_code, stock_data in stocks_data.items():
            for window_type, metrics in stock_data.items():
                rows.append([
                    time_str, stock_code, window_type, 
                    metrics.get('mean', 0.0), metrics.get('std', 0.0)
                ])
    
    df = pd.DataFrame(rows, columns=['time', 'stock_code', 'window_type', 'mean', 'std'])
    
    csv_gz_path = output_path.replace('.json', '.csv.gz')
    df.to_csv(csv_gz_path, compression='gzip', index=False)
    print(f"ğŸ’¾ CSV.gzæ–‡ä»¶ä¿å­˜è‡³: {csv_gz_path}")
    return csv_gz_path

def save_data_as_pickle_gz(final_data: dict, output_path: str):
    """å°†æ•°æ®ä¿å­˜ä¸ºå‹ç¼©Pickleæ ¼å¼"""
    pickle_gz_path = output_path.replace('.json', '.pkl.gz')
    
    with gzip.open(pickle_gz_path, 'wb') as f:
        pickle.dump(final_data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    print(f"ğŸ’¾ Pickle.gzæ–‡ä»¶ä¿å­˜è‡³: {pickle_gz_path}")
    return pickle_gz_path

def load_data_from_pickle_gz(pickle_gz_path: str) -> dict:
    """ä»Pickle.gzæ–‡ä»¶è¯»å–æ•°æ®"""
    with gzip.open(pickle_gz_path, 'rb') as f:
        return pickle.load(f)

def clean_old_output_files(output_dir: str, current_file: str, keep_count: int = 2):
    """åˆ é™¤æ—§çš„è¾“å‡ºæ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„å‡ ä¸ª - ä¿®æ”¹ä¸ºæ”¯æŒparquet"""
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ time_data_*.parquet æ–‡ä»¶
        pattern = os.path.join(output_dir, "time_data_*.parquet")
        all_files = glob.glob(pattern)
        
        # æ’é™¤å½“å‰æ–‡ä»¶
        other_files = [f for f in all_files if f != current_file]
        
        if not other_files:
            print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°å†å²è¾“å‡ºæ–‡ä»¶")
            return
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨åé¢
        other_files.sort(key=os.path.getmtime)
        
        # å¦‚æœæ–‡ä»¶æ•°é‡è¶…è¿‡ä¿ç•™æ•°é‡ï¼Œåˆ é™¤å¤šä½™çš„æ—§æ–‡ä»¶
        if len(other_files) >= keep_count:
            files_to_delete = other_files[:-keep_count+1]  # ä¿ç•™ keep_count-1 ä¸ªæ—§æ–‡ä»¶
            
            for file_path in files_to_delete:
                try:
                    os.remove(file_path)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {os.path.basename(file_path)}")
                except OSError as e:
                    print(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")
        
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†æ—§æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# import pandas as pd
# import numpy as np
# from datetime import datetime
# from functools import partial
# from concurrent.futures import ProcessPoolExecutor, as_completed
# from tqdm import tqdm
# import json
# import os

# # ======================== é…ç½® ========================
# WINDOW_LENGTH_LIST = [1, 3, 5]   # æµ‹è¯•æ»šåŠ¨çª—å£
# INCLUDE_FULL_ROLLING = True       # æ˜¯å¦åŒ…å«å…¨é‡æ»šåŠ¨
# NUM_PROCESSES = 2                 # å¹¶è¡Œè¿›ç¨‹æ•°
# SAVE_PATH = "./test_output"       # CSV ä¿å­˜è·¯å¾„
# os.makedirs(SAVE_PATH, exist_ok=True)


# def generate_test_data(num_stocks=3, num_minutes=5):
#     dfs = []
#     for i in range(num_stocks):
#         ts_code = f"{i:06d}"
#         trade_time = pd.date_range('2025-10-17 09:30:00', periods=num_minutes, freq='1T')
#         df = pd.DataFrame({
#             'trade_date': ['2025-10-17'] * num_minutes,
#             'ts_code': [ts_code] * num_minutes,
#             'trade_time': trade_time,
#             'time_only': trade_time.time,
#             'vol': np.random.randint(100, 500, size=num_minutes)
#         })
#         dfs.append(df)
#     return pd.concat(dfs).reset_index(drop=True)

# ======================== CSV è¾“å‡ºå‡½æ•° ========================
def save_rolling_to_csv(rolling_result, save_path):
    os.makedirs(save_path, exist_ok=True)
    
    for trade_date, stocks_data in rolling_result.items():
        rows = []
        for ts_code, stock_data in stocks_data.items():
            row = {'ts_code': ts_code}
            for k, v in stock_data.items():
                if k == 'rolling_full':
                    # rolling_full æ˜¯å¸¦ mean å’Œ std çš„å­—å…¸
                    for sub_k, sub_v in v['mean'].items():
                        row[f"{k}_mean_{sub_k}"] = sub_v
                    for sub_k, sub_v in v['std'].items():
                        row[f"{k}_std_{sub_k}"] = sub_v
                else:
                    # æ™®é€š rolling çª—å£
                    for time_k, val in v.items():
                        row[f"{k}_{time_k}"] = val
            rows.append(row)
        df = pd.DataFrame(rows)
        file_path = os.path.join(save_path, f"rolling_{trade_date}.csv")
        df.to_csv(file_path, index=False)
        print(f"ğŸ’¾ å·²ä¿å­˜: {file_path}")

# # ======================== æµ‹è¯•è„šæœ¬ ========================
# if __name__ == "__main__":
#     df_test = generate_test_data(num_stocks=3, num_minutes=5)
    
#     # å•æ‰¹æ¬¡æµ‹è¯•
#     test_chunk = [('2025-10-17', '000000', df_test[df_test['ts_code']=='000000'])]
#     single_result = process_stock_chunk(test_chunk, WINDOW_LENGTH_LIST, INCLUDE_FULL_ROLLING)
#     print("=== å•æ‰¹æ¬¡ç»“æœ ===")
#     print(json.dumps(single_result, indent=2, ensure_ascii=False))
    
#     # å¹¶è¡Œè®¡ç®—æµ‹è¯•
#     parallel_result = calculate_rolling_data_parallel_optimized(df_test)
#     print("=== å¹¶è¡Œè®¡ç®—ç»“æœ ===")
#     print(json.dumps(parallel_result, indent=2, ensure_ascii=False))
    
#     # ä¿å­˜ CSV
#     save_rolling_to_csv(parallel_result, SAVE_PATH)

def main():
    """ä¼˜åŒ–åçš„ä¸»å‡½æ•°"""
    print("=== å¼€å§‹æ•°æ®å¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰===")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶æ‰©å±•å
    output_path = os.path.join(OUTPUT_DIR, f"time_data_{TARGET_DATE}.parquet")
    
    if os.path.exists(output_path):
        print(f"âœ… è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨: {os.path.basename(output_path)}")
        print("âŒ è·³è¿‡å¤„ç†")
        return
    
    print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {os.path.basename(output_path)}")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨ {NUM_PROCESSES} ä¸ªCPUè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†")
    
    try:
        # æ­¥éª¤1: è·å–å’Œé¢„å¤„ç†æ•°æ®
        print("ğŸ“Š è·å–åŸå§‹æ•°æ®...")
        raw_data = get_stock_data()
        
        print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
        df = preprocess_stock_minute_data(raw_data)
        
        print("âš¡ å¹¶è¡Œè®¡ç®—æ»šåŠ¨æ•°æ®...")
        rolling_data = calculate_rolling_data_parallel_optimized(df)
        
        # æ­¥éª¤2: å¹¶è¡Œå¤„ç†ç»Ÿè®¡æ•°æ®
        print("ğŸ“ˆ å¹¶è¡Œå¤„ç†ç»Ÿè®¡æ•°æ®...")
        stats_data = process_statistics_data_optimized(rolling_data, TARGET_DATE, DATE_INTERVAL)
        
        # æ­¥éª¤3: å¹¶è¡Œè½¬æ¢ä¸ºæ—¶é—´åºåˆ—æ ¼å¼
        print("ğŸ”„ å¹¶è¡Œè½¬æ¢æ—¶é—´åºåˆ—æ ¼å¼...")
        final_data = convert_to_time_format_parallel(stats_data)
        
        # æ­¥éª¤4: ä¿å­˜æœ€ç»ˆç»“æœ (ä¿®æ”¹è¿™éƒ¨åˆ†)
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœ...")
        final_path = save_data_as_parquet(final_data, output_path)
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ")
        print(f"åŒ…å« {len(final_data)} ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®")
        
        # æ­¥éª¤5: æ¸…ç†æ—§æ–‡ä»¶ (éœ€è¦ä¿®æ”¹pattern)
        print("ğŸ§¹ æ¸…ç†å†å²æ–‡ä»¶...")
        clean_old_output_files(OUTPUT_DIR, final_path)
        
        print("=== å¤„ç†å®Œæˆ ===")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if os.path.exists(output_path):
            try:
                os.remove(output_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸å®Œæ•´çš„è¾“å‡ºæ–‡ä»¶")
            except:
                pass
        raise

# ======================== æµ‹è¯•è„šæœ¬ ========================
if __name__ == "__main__":
    main()