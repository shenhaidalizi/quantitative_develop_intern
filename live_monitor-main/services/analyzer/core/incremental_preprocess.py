"""
å¢é‡é¢„å¤„ç†ï¼šåªå¤„ç†æ–°å¢çš„æ—¥æœŸæ•°æ®
é¿å…æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—å…¨éƒ¨30å¤©çš„å†å²æ•°æ®

æ³¨æ„ï¼šç»§æ‰¿äº† preprocess_data.py çš„ DATA_ROOT é…ç½®
     OUTPUT_DIR ä¼šè‡ªåŠ¨ä½¿ç”¨ $DATA_ROOT/statistic_data
"""

import os
import glob
import time
from datetime import datetime, timedelta
from preprocess_data import *

def get_latest_processed_date(output_dir: str) -> str:
    """è·å–æœ€è¿‘ä¸€æ¬¡å¤„ç†çš„æ—¥æœŸ"""
    pattern = os.path.join(output_dir, "time_data_*.parquet")
    files = glob.glob(pattern)
    
    if not files:
        return None
    
    latest_file = max(files, key=os.path.getmtime)
    filename = os.path.basename(latest_file)
    date_str = filename.replace('time_data_', '').replace('.parquet', '')
    
    return date_str


def incremental_main():
    """å¢é‡å¤„ç†ä¸»å‡½æ•°"""
    print("=== å¢é‡æ•°æ®å¤„ç†æ¨¡å¼ ===")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_path = os.path.join(OUTPUT_DIR, f"time_data_{TARGET_DATE}.parquet")
    
    # æ£€æŸ¥ä»Šå¤©çš„æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(output_path):
        print(f"âœ… ä»Šæ—¥æ•°æ®å·²å­˜åœ¨: {os.path.basename(output_path)}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡1å°æ—¶ï¼‰
        file_age = time.time() - os.path.getmtime(output_path)
        if file_age < 3600:  # 1å°æ—¶å†…
            print(f"â±ï¸ æ–‡ä»¶åˆ›å»ºäº {file_age/60:.1f} åˆ†é’Ÿå‰ï¼Œè·³è¿‡å¤„ç†")
            return
        else:
            print(f"âš ï¸ æ–‡ä»¶å·²è¶…è¿‡1å°æ—¶ï¼Œå°†é‡æ–°ç”Ÿæˆ")
    
    try:
        # æ­¥éª¤1: è·å–å’Œé¢„å¤„ç†æ•°æ®
        print("ğŸ“Š è·å–åŸå§‹æ•°æ®...")
        raw_data = get_stock_data()
        
        # åªå¤„ç†æœ€è¿‘DATE_INTERVALå¤©çš„æ•°æ®
        print(f"ğŸ”„ é¢„å¤„ç†æ•°æ®ï¼ˆæœ€è¿‘{DATE_INTERVAL}å¤©ï¼‰...")
        df = preprocess_stock_minute_data(raw_data)
        
        # è¿‡æ»¤æœ€è¿‘Nå¤©çš„æ•°æ®
        cutoff_date = pd.to_datetime(TARGET_DATE) - timedelta(days=DATE_INTERVAL)
        df = df[df['trade_date'] >= cutoff_date.date()]
        print(f"ğŸ“… è¿‡æ»¤åæ•°æ®é‡: {len(df)} æ¡è®°å½•")
        
        print("âš¡ å¹¶è¡Œè®¡ç®—æ»šåŠ¨æ•°æ®...")
        rolling_data = calculate_rolling_data_parallel_optimized(df)
        
        print("ğŸ“ˆ å¹¶è¡Œå¤„ç†ç»Ÿè®¡æ•°æ®...")
        stats_data = process_statistics_data_optimized(rolling_data, TARGET_DATE, DATE_INTERVAL)
        
        print("ğŸ”„ å¹¶è¡Œè½¬æ¢æ—¶é—´åºåˆ—æ ¼å¼...")
        final_data = convert_to_time_format_parallel(stats_data)
        
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœ...")
        final_path = save_data_as_parquet(final_data, output_path)
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ")
        print(f"åŒ…å« {len(final_data)} ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®")
        
        print("ğŸ§¹ æ¸…ç†å†å²æ–‡ä»¶...")
        clean_old_output_files(OUTPUT_DIR, final_path, KEEP_FILE_COUNT)
        
        print("=== å¤„ç†å®Œæˆ ===")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if os.path.exists(output_path):
            try:
                os.remove(output_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸å®Œæ•´çš„è¾“å‡ºæ–‡ä»¶")
            except:
                pass
        raise


if __name__ == "__main__":
    incremental_main()
